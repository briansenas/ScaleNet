{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "import sys\n",
    "import os, sys, inspect\n",
    "\n",
    "current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "sys.path.insert(0, current_dir)\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tensorboardX import SummaryWriter\n",
    "from models.model_RCNN_only import RCNN_only\n",
    "from maskrcnn_rui.data.transforms import build_transforms_maskrcnn\n",
    "from maskrcnn_rui.config import cfg\n",
    "import utils.model_utils as model_utils\n",
    "from utils.logger import setup_logger, printer\n",
    "from maskrcnn_benchmark.utils.comm import synchronize, get_rank\n",
    "from utils.checkpointer import DetectronCheckpointer\n",
    "from utils.utils_misc import colored\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from dataset_cvpr import bins2roll, bins2vfov, bins2horizon, bins2pitch\n",
    "from panorama_cropping_dataset_generation.debugging import showHorizonLine\n",
    "import random\n",
    "\n",
    "seed = 140421\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Rui's Scale Estimation Network Training\")\n",
    "# Training\n",
    "parser.add_argument(\"--task_name\", type=str, default=\"tmp\", help=\"resume training\")\n",
    "parser.add_argument(\n",
    "    \"--workers\", type=int, help=\"number of data loading workers\", default=8\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batchsize\", type=int, default=36, help=\"input batch size during training\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--save_every_iter\",\n",
    "    type=int,\n",
    "    default=0,\n",
    "    help=\"set to 0 to save ONLY at the end of each epoch\",\n",
    ")\n",
    "# parser.add_argument('--batchsizeeval', type=int, default=42, help='input batch size during evaluation')\n",
    "parser.add_argument(\n",
    "    \"--niter\", type=int, default=5000, help=\"number of epochs to train for\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lr\", type=float, default=1e-3, help=\"learning rate, default=0.005\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--beta1\", type=float, default=0.9, help=\"beta1 for adam. default=0.5\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--not_val\", action=\"store_true\", help=\"Do not validate duruign training\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--save_every_epoch\", type=int, default=10, help=\"save checkpoint every ? epoch\"\n",
    ")\n",
    "# Model\n",
    "parser.add_argument(\n",
    "    \"--accu_model\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Use accurate model with theta instead of Derek's approx.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--est_kps\", action=\"store_true\", help=\"Enable estimating keypoints\"\n",
    ")\n",
    "# Pretraining\n",
    "parser.add_argument(\n",
    "    \"--resume\",\n",
    "    type=str,\n",
    "    help=\"resume training; can be full path (e.g. tmp/checkpoint0.pth.tar) or taskname (e.g. tmp)\",\n",
    "    default=\"NoCkpt\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--feature_only\",\n",
    "    action=\"store_true\",\n",
    "    help=\"restore only features (remove all classifiers) from checkpoint\",\n",
    ")\n",
    "# Device\n",
    "parser.add_argument(\"--cpu\", action=\"store_true\", help=\"Force training on CPU\")\n",
    "parser.add_argument(\"--rank\", type=int, default=0)\n",
    "parser.add_argument(\"--master_port\", type=str, default=\"8914\")\n",
    "# DEBUG\n",
    "parser.add_argument(\"--debug\", action=\"store_true\", help=\"Debug eval\")\n",
    "# Mask R-CNN\n",
    "parser.add_argument(\"--not_rcnn\", action=\"store_true\", help=\"Disable Mask R-CNN module\")\n",
    "\n",
    "parser.add_argument(\"--pointnet_camH\", action=\"store_true\", help=\"\")\n",
    "parser.add_argument(\n",
    "    \"--est_bbox\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Enable estimating bboxes instead of using GT bboxes\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--config-file\",\n",
    "    default=\"\",\n",
    "    metavar=\"FILE\",\n",
    "    help=\"path to config file\",\n",
    "    type=str,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"opts\",\n",
    "    help=\"Modify config options using the command-line\",\n",
    "    default=None,\n",
    "    nargs=argparse.REMAINDER,\n",
    ")\n",
    "\n",
    "# opt = parser.parse_args()\n",
    "opt = parser.parse_args(\n",
    "    \"--batchsize=32 --task_name tmp_eval --niter 1 --accu_model --resume YES \\\n",
    "--config-file config/coco_config_small_RCNNOnly.yaml \\\n",
    "SOLVER.IMS_PER_BATCH 1 TEST.IMS_PER_BATCH 1\".split()\n",
    ")\n",
    "\n",
    "opt.checkpoints_folder = \"checkpoint\"\n",
    "\n",
    "# config_file = \"maskrcnn/coco_config.yaml\"\n",
    "config_file = opt.config_file\n",
    "cfg.merge_from_file(config_file)\n",
    "# manual override some options\n",
    "cfg.merge_from_list([\"MODEL.DEVICE\", \"cuda\"])\n",
    "cfg.merge_from_list(opt.opts)\n",
    "cfg.freeze()\n",
    "opt.cfg = cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DISTRIBUTED TRAINING\n",
    "num_gpus = int(os.environ[\"WORLD_SIZE\"]) if \"WORLD_SIZE\" in os.environ else 1\n",
    "opt.distributed = num_gpus > 1\n",
    "if opt.distributed:\n",
    "    torch.cuda.set_device(opt.rank)\n",
    "    torch.distributed.init_process_group(backend=\"nccl\", init_method=\"env://\")\n",
    "    synchronize()\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() and not opt.cpu else \"cpu\")\n",
    "device = \"cuda\"\n",
    "print(\"Device:\", device)\n",
    "rank = get_rank()\n",
    "\n",
    "# === SUMMARY WRITERS\n",
    "summary_path = \"./summary/\" + opt.task_name\n",
    "writer = SummaryWriter(summary_path)\n",
    "\n",
    "# === LOGGING\n",
    "# sys.stdout = Logger(summary_path+'/log.txt')\n",
    "logger = setup_logger(\n",
    "    \"logger:train\", summary_path, get_rank(), filename=\"logger_maskrcn-style.txt\"\n",
    ")\n",
    "logger.info(colored(\"==[config]== opt\", \"white\", \"on_blue\"))\n",
    "logger.info(opt)\n",
    "logger.info(colored(\"==[config]== cfg\", \"white\", \"on_blue\"))\n",
    "logger.info(cfg)\n",
    "logger.info(\n",
    "    colored(\n",
    "        \"==[config]== Loaded configuration file {}\".format(opt.config_file),\n",
    "        \"white\",\n",
    "        \"on_blue\",\n",
    "    )\n",
    ")\n",
    "with open(opt.config_file, \"r\") as cf:\n",
    "    config_str = \"\\n\" + cf.read()\n",
    "    logger.info(config_str)\n",
    "printer = printer(get_rank(), debug=opt.debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_task_name = \"1109-0141-mm1_SUN360RCNN-HorizonPitchRollVfovNET_myDistNarrowerLarge1105_bs16on4_le1e-5_indeptClsHeads_synBNApex_valBS1_yannickTransformAug\"\n",
    "\n",
    "model = RCNN_only(cfg, opt, logger, printer, rank=rank)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), lr=cfg.SOLVER.BASE_LR, betas=(opt.beta1, 0.999), eps=1e-5\n",
    ")\n",
    "scheduler = ReduceLROnPlateau(optimizer, \"min\", factor=0.1, patience=20, cooldown=10)\n",
    "\n",
    "save_to_disk = get_rank() == 0\n",
    "opt.checkpoints_folder = \"checkpoint\"\n",
    "checkpointer = DetectronCheckpointer(\n",
    "    opt,\n",
    "    model,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    opt.checkpoints_folder,\n",
    "    os.path.join(opt.checkpoints_folder, resume_task_name),\n",
    "    save_to_disk,\n",
    "    logger=logger,\n",
    ")\n",
    "checkpoint_restored, _, _ = checkpointer.load(task_name=resume_task_name)\n",
    "\n",
    "# === DATASET\n",
    "train_trnfs_maskrcnn = build_transforms_maskrcnn(cfg, True)\n",
    "eval_trnfs_maskrcnn = build_transforms_maskrcnn(cfg, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Files for training ScaleNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_coco_pickle_noYannickMat import my_collate, COCO2017Scale\n",
    "from utils.data_utils import make_data_loader\n",
    "# === DATASET\n",
    "train_trnfs_maskrcnn = build_transforms_maskrcnn(cfg, True)\n",
    "\n",
    "ds_train_coco_vis = COCO2017Scale(\n",
    "    transforms_maskrcnn=train_trnfs_maskrcnn,\n",
    "    split=\"train\",\n",
    "    shuffle=False,\n",
    "    logger=logger,\n",
    "    opt=opt,\n",
    "    coco_subset=\"coco_scale_eccv\",\n",
    ")  # !!!!!!!\n",
    "training_loader_coco_vis = make_data_loader(\n",
    "    cfg,\n",
    "    ds_train_coco_vis,\n",
    "    is_train=False,\n",
    "    is_distributed=False,\n",
    "    start_iter=0,\n",
    "    logger=logger,\n",
    "    collate_fn=my_collate,\n",
    "    batch_size_override=16,  # BN does not make sense when model.train() and batchsize==1!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "results_path_yannick = \"data/COCO/coco_results/yannick_results_train2017_filtered\"\n",
    "os.makedirs(results_path_yannick, exist_ok=True)\n",
    "from scipy.io import savemat\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (\n",
    "        inputCOCO_Image_maskrcnnTransform_list,\n",
    "        W_batch_array,\n",
    "        H_batch_array,\n",
    "        yc_batch,\n",
    "        bboxes_batch_array,\n",
    "        bboxes_length_batch_array,\n",
    "        v0_batch,\n",
    "        f_pixels_yannick_batch,\n",
    "        im_filename,\n",
    "        im_file,\n",
    "        target_maskrcnnTransform_list,\n",
    "        labels_list,\n",
    "        _,\n",
    "    ) in tqdm(enumerate(training_loader_coco_vis)):\n",
    "        list_of_oneLargeBbox_list_cpu = model_utils.oneLargeBboxList(W_batch_array, H_batch_array)\n",
    "        list_of_oneLargeBbox_list = [\n",
    "            bbox_list_array.to(device) for bbox_list_array in list_of_oneLargeBbox_list_cpu\n",
    "        ]\n",
    "        output_RCNN = model(\n",
    "            image_batch_list=inputCOCO_Image_maskrcnnTransform_list,\n",
    "            list_of_oneLargeBbox_list=list_of_oneLargeBbox_list,\n",
    "        )\n",
    "        output_horizon = output_RCNN[\"output_horizon\"]\n",
    "        output_pitch = output_RCNN[\"output_pitch\"]\n",
    "        output_roll = output_RCNN[\"output_roll\"]\n",
    "        output_vfov = output_RCNN[\"output_vfov\"]\n",
    "        for idx in range(len(output_horizon)):\n",
    "            horizon_disc = output_horizon[idx].detach().cpu().numpy().squeeze()\n",
    "            pitch_disc = output_pitch[idx].detach().cpu().numpy().squeeze()\n",
    "            roll_disc = output_roll[idx].detach().cpu().numpy().squeeze()\n",
    "            vfov_disc = output_vfov[idx].detach().cpu().numpy().squeeze()\n",
    "            vfov_disc[..., 0] = -35\n",
    "            vfov_disc[..., -1] = -35\n",
    "            horizon = bins2horizon(horizon_disc)\n",
    "            pitch = bins2pitch(pitch_disc)\n",
    "            roll = bins2roll(roll_disc)\n",
    "            vfov = bins2vfov(vfov_disc)\n",
    "            filename = os.path.basename(im_filename[idx])\n",
    "            filepath = os.path.join(results_path_yannick, filename+\".mat\")\n",
    "            savemat(filepath, dict(horizon=horizon, pitch=pitch, roll=roll, vfov=vfov), appendmat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # 很关键！\n",
    "\n",
    "test_image_path = \"demo/fall-cmu-700x700.jpg\"\n",
    "# test_image_path = 'demo/white-house.jpg'\n",
    "im_ori_RGB = Image.open(test_image_path).convert(\"RGB\")  # im_ori_RGB.size: [W, H]\n",
    "im = eval_trnfs_maskrcnn(im_ori_RGB)\n",
    "\n",
    "H_num, W_num = im_ori_RGB.size\n",
    "list_of_oneLargeBbox_list_cpu = model_utils.oneLargeBboxList([W_num], [H_num])\n",
    "list_of_oneLargeBbox_list = [\n",
    "    bbox_list_array.to(device) for bbox_list_array in list_of_oneLargeBbox_list_cpu\n",
    "]\n",
    "\n",
    "output_RCNN = model(\n",
    "    image_batch_list=[im.to(device)],\n",
    "    list_of_oneLargeBbox_list=list_of_oneLargeBbox_list,\n",
    ")\n",
    "output_horizon = output_RCNN[\"output_horizon\"]\n",
    "output_pitch = output_RCNN[\"output_pitch\"]\n",
    "output_roll = output_RCNN[\"output_roll\"]\n",
    "output_vfov = output_RCNN[\"output_vfov\"]\n",
    "\n",
    "idx = 0\n",
    "\n",
    "im = im_ori_RGB\n",
    "if len(im.getbands()) == 1:\n",
    "    im = Image.fromarray(np.tile(np.asarray(im)[:, :, np.newaxis], (1, 1, 3)))\n",
    "\n",
    "horizon_disc = output_horizon[idx].detach().cpu().numpy().squeeze()\n",
    "pitch_disc = output_pitch[idx].detach().cpu().numpy().squeeze()\n",
    "roll_disc = output_roll[idx].detach().cpu().numpy().squeeze()\n",
    "vfov_disc = output_vfov[idx].detach().cpu().numpy().squeeze()\n",
    "vfov_disc[..., 0] = -35\n",
    "vfov_disc[..., -1] = -35\n",
    "\n",
    "horizon = bins2horizon(horizon_disc)\n",
    "pitch = bins2pitch(pitch_disc)\n",
    "roll = bins2roll(roll_disc)\n",
    "vfov = bins2vfov(vfov_disc)\n",
    "w, h = im.size\n",
    "f_pix = h / 2.0 / np.tan(vfov / 2.0)\n",
    "# sensor_size = sensor_size_num[idx]\n",
    "sensor_size = 24  # !!!!!!\n",
    "f_mm = f_pix / h * sensor_size\n",
    "\n",
    "im2, _ = showHorizonLine(\n",
    "    np.asarray(im).copy(),\n",
    "    vfov,\n",
    "    pitch,\n",
    "    roll,\n",
    "    focal_length=f_mm,\n",
    "    debug=True,\n",
    "    color=(0, 0, 255),\n",
    "    width=3,\n",
    ")  # Blue: horizon converted from camera params with roll\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(im2)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im.size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

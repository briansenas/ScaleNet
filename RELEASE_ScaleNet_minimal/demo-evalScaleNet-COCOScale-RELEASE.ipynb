{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "\n",
    "import os,sys,inspect\n",
    "current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "print(current_dir)\n",
    "sys.path.insert(0, current_dir)\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "torch_version = torch.__version__\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from models.model_RCNNOnly_combine_indeptPointnet_maskrcnnPose_discount import RCNNOnly_combine\n",
    "from dataset_coco_pickle_noYannickMat import my_collate, COCO2017Scale\n",
    "from utils.data_utils import make_data_loader\n",
    "from utils.utils_misc import colored\n",
    "from maskrcnn_rui.data.transforms import build_transforms_maskrcnn, build_transforms_yannick\n",
    "from utils.logger import setup_logger, printer\n",
    "\n",
    "from maskrcnn_rui.config import cfg\n",
    "from maskrcnn_rui.utils.comm import get_rank\n",
    "import utils.vis_utils as vis_utils\n",
    "from utils.checkpointer import DetectronCheckpointer\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Rui's Scale Estimation Network Training\")\n",
    "# Training\n",
    "parser.add_argument('--task_name', type=str, default='tmp', help='resume training')\n",
    "parser.add_argument('--workers', type=int, help='number of data loading workers', default=8)\n",
    "parser.add_argument('--save_every_iter', type=int, default=0, help='set to 0 to save ONLY at the end of each epoch')\n",
    "parser.add_argument('--summary_every_iter', type=int, default=20, help='')\n",
    "parser.add_argument('--nepoch', type=int, default=15000, help='number of epochs to train for')\n",
    "parser.add_argument('--beta1', type=float, default=0.9, help='beta1 for adam. default=0.5')\n",
    "parser.add_argument('--not_val', action='store_true', help='Do not validate duruign training')\n",
    "parser.add_argument('--not_vis', action='store_true', help='')\n",
    "parser.add_argument('--not_vis_SUN360', action='store_true', help='')\n",
    "parser.add_argument('--save_every_epoch', type=int, default=10, help='save checkpoint every ? epoch')\n",
    "parser.add_argument('--vis_every_epoch', type=int, default=5, help='vis every ? epoch')\n",
    "# Model\n",
    "parser.add_argument('--accu_model', action='store_true', help='Use accurate model with theta instead of Derek\\'s approx.')\n",
    "parser.add_argument('--argmax_val', action='store_true', help='')\n",
    "parser.add_argument('--direct_camH', action='store_true', help='direct preidict one number for camera height ONLY, instead of predicting a distribution')\n",
    "parser.add_argument('--direct_v0', action='store_true', help='direct preidict one number for v0 ONLY, instead of predicting a distribution')\n",
    "parser.add_argument('--direct_fmm', action='store_true', help='direct preidict one number for fmm ONLY, instead of predicting a distribution')\n",
    "\n",
    "# Pre-training\n",
    "parser.add_argument('--resume', type=str, help='resume training; can be full path (e.g. tmp/checkpoint0.pth.tar) or taskname (e.g. tmp)', default='NoCkpt')\n",
    "parser.add_argument('--feature_only', action='store_true', help='restore only features (remove all classifiers) from checkpoint')\n",
    "parser.add_argument('--reset_scheduler', action='store_true', help='') # NOT working yet\n",
    "parser.add_argument('--reset_lr', action='store_true', help='') # NOT working yet\n",
    "\n",
    "# Device\n",
    "parser.add_argument('--cpu', action='store_true', help='Force training on CPU')\n",
    "parser.add_argument(\"--local_rank\", type=int, default=0)\n",
    "parser.add_argument(\"--master_port\", type=str, default='8914')\n",
    "\n",
    "# DEBUG\n",
    "parser.add_argument('--debug', action='store_true', help='Debug eval')\n",
    "parser.add_argument('--debug_memory', action='store_true', help='Debug eval')\n",
    "\n",
    "# Mask R-CNN\n",
    "## Modules\n",
    "parser.add_argument('--train_cameraCls', action='store_true', help='Disable camera calibration network')\n",
    "parser.add_argument('--train_roi_h', action='store_true', help='')\n",
    "parser.add_argument('--est_bbox', action='store_true', help='Enable estimating bboxes instead of using GT bboxes')\n",
    "parser.add_argument('--est_kps', action='store_true', help='Enable estimating keypoints')\n",
    "parser.add_argument('--if_discount', action='store_true', help='')\n",
    "parser.add_argument('--discount_from', type=str, default='GT') # ('GT', 'pred')\n",
    "\n",
    "## Losses\n",
    "parser.add_argument('--loss_last_layer', action='store_true', help='Using loss of last layer only')\n",
    "parser.add_argument('--loss_person_all_layers', action='store_true', help='Using loss of last layer only')\n",
    "parser.add_argument('--not_rcnn', action='store_true', help='Disable Mask R-CNN person height bbox head')\n",
    "parser.add_argument('--no_kps_loss', action='store_true', help='')\n",
    "\n",
    "## Archs\n",
    "parser.add_argument('--pointnet_camH', action='store_true', help='')\n",
    "parser.add_argument('--pointnet_camH_refine', action='store_true', help='')\n",
    "parser.add_argument('--pointnet_personH_refine', action='store_true', help='')\n",
    "parser.add_argument('--pointnet_roi_feat_input', action='store_true', help='') # NOT working yet\n",
    "parser.add_argument('--pointnet_roi_feat_input_person3', action='store_true', help='') # NOT working yet\n",
    "parser.add_argument('--pointnet_fmm_refine', action='store_true', help='') # NOT working yet\n",
    "parser.add_argument('--pointnet_v0_refine', action='store_true', help='') # NOT working yet\n",
    "parser.add_argument('--not_pointnet_detach_input', action='store_true', help='') # NOT working yet\n",
    "parser.add_argument(\"--num_layers\", type=int, default=3)\n",
    "parser.add_argument('--fit_derek', action='store_true', help='')\n",
    "## weights\n",
    "parser.add_argument('--weight_SUN360', type=float, default=10., help='weight for Yannick\\'s losses. default=1.')\n",
    "parser.add_argument('--weight_kps', type=float, default=1e-3, help='weight for Yannick\\'s losses. default=1.')\n",
    "\n",
    "## debug\n",
    "parser.add_argument('--zero_pitch', action='store_true', help='') # NOT working yet\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--config-file\",\n",
    "    default=\"\",\n",
    "    metavar=\"FILE\",\n",
    "    help=\"path to config file\",\n",
    "    type=str,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"opts\",\n",
    "    help=\"Modify config options using the command-line\",\n",
    "    default=None,\n",
    "    nargs=argparse.REMAINDER,\n",
    ")\n",
    "\n",
    "# [2.1-SN-L3] 20200222-162430_pod_backCompat_adam_wPerson05_720-540_REafterDeathV_afterFaster_bs16_fix3_nokpsLoss_personLoss3Layers_loss3layers\n",
    "opt = parser.parse_args('--task_name tmp_eval --num_layers 3 \\\n",
    "--train_cameraCls --train_roi_h --pointnet_camH --pointnet_camH_refine --pointnet_personH_refine --accu_model --no_kps_loss --loss_person_all_layers \\\n",
    "--config-file coco_config_small_synBN1108_kps.yaml  --weight_SUN360=10. \\\n",
    "MODEL.LOSS.VT_LOSS_CLAMP 2. SOLVER.IMS_PER_BATCH 16 TEST.IMS_PER_BATCH 16 SOLVER.PERSON_WEIGHT 0.05 SOLVER.BASE_LR 1e-5 MODEL.HUMAN.MEAN 1.70 MODEL.HUMAN.STD 0.15 MODEL.RCNN_WEIGHT_BACKBONE 0212-2047_mm2_SUN360RCNN_LossposeCamera_lr1e-5_NOsubsample-prepare_targets_for_gt_box_input-removeOutOfBox_wKps1-SUN10_RE MODEL.RCNN_WEIGHT_CLS_HEAD 0212-2047_mm2_SUN360RCNN_LossposeCamera_lr1e-5_NOsubsample-prepare_targets_for_gt_box_input-removeOutOfBox_wKps1-SUN10_RE MODEL.RCNN_WEIGHT_KPS_HEAD 0212-2047_mm2_SUN360RCNN_LossposeCamera_lr1e-5_NOsubsample-prepare_targets_for_gt_box_input-removeOutOfBox_wKps1-SUN10_RE'\\\n",
    "                       .split())\n",
    "\n",
    "print(opt)\n",
    "                        \n",
    "opt.checkpoints_folder = 'checkpoint'\n",
    "\n",
    "config_file = opt.config_file\n",
    "cfg.merge_from_file(config_file)\n",
    "# manual override some options\n",
    "cfg.merge_from_list([\"MODEL.DEVICE\", \"cuda\"])\n",
    "cfg.merge_from_list(opt.opts)\n",
    "cfg.freeze()\n",
    "\n",
    "opt.cfg = cfg\n",
    "\n",
    "# sys.path.insert(0, cfg.MODEL.POINTNET.PATH)\n",
    "opt.rank = opt.local_rank\n",
    "\n",
    "num_gpus = 1\n",
    "opt.distributed = num_gpus > 1\n",
    "device = 'cuda'\n",
    "opt.device = device\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# === SUMMARY WRITERS\n",
    "summary_path = './summary/'+opt.task_name\n",
    "writer = SummaryWriter(summary_path)\n",
    "\n",
    "# === LOGGING\n",
    "logger = setup_logger(\"logger:train\", summary_path, get_rank(), filename=\"logger_maskrcn-style.txt\")\n",
    "logger.info(colored(\"==[config]== opt\", 'white', 'on_blue'))\n",
    "logger.info(opt)\n",
    "logger.info(colored(\"==[config]== cfg\", 'white', 'on_blue'))\n",
    "logger.info(cfg)\n",
    "logger.info(colored(\"==[config]== Loaded configuration file {}\".format(opt.config_file), 'white', 'on_blue'))\n",
    "with open(opt.config_file, \"r\") as cf:\n",
    "    config_str = \"\\n\" + cf.read()\n",
    "    logger.info(config_str)\n",
    "printer = printer(get_rank(), debug=opt.debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MODEL & TRAINING\n",
    "modules_not_build = []\n",
    "if not opt.train_cameraCls:\n",
    "    modules_not_build.append('classifier_heads')\n",
    "if not opt.train_roi_h:\n",
    "    modules_not_build.append('roi_h_heads')\n",
    "if not opt.est_bbox and not opt.est_kps:\n",
    "    modules_not_build.append('roi_bbox_heads')\n",
    "sys.path.insert(0, 'models/pointnet')\n",
    "model = RCNNOnly_combine(opt, logger, printer, num_layers=opt.num_layers, modules_not_build=modules_not_build)\n",
    "\n",
    "# model.print_net()\n",
    "# model.init_restore()\n",
    "# model.set_train_params()\n",
    "model.to(device)\n",
    "model.turn_on_all_params()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    printer.print(name, param.shape, param.requires_grad)\n",
    "printer.print('ALL %d params'%len(list(model.named_parameters())))\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=cfg.SOLVER.BASE_LR, betas=(opt.beta1, 0.999), eps=1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=100, cooldown=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CHECKPOINT\n",
    "\n",
    "# [1] 20200221-120910_pod_firstKpsAndRest_batchsize8_KaimingInit_adam_wPerson05_wKPS1e-3_720-540_REafterDeathV_afterFaster_bs16_fix3_DISCOUNTfromPredkps__personLoss3Layers\n",
    "# opt.resume = '20200221-120910_pod_firstKpsAndRest_batchsize8_KaimingInit_adam_wPerson05_wKPS1e-3_720-540_REafterDeathV_afterFaster_bs16_fix3_DISCOUNTfromPredkps__personLoss3Layers'\n",
    "# [2.1] 20200222-162430_pod_backCompat_adam_wPerson05_720-540_REafterDeathV_afterFaster_bs16_fix3_nokpsLoss_personLoss3Layers_loss3layers\n",
    "# opt.resume = '20200222-162430_pod_backCompat_adam_wPerson05_720-540_REafterDeathV_afterFaster_bs16_fix3_nokpsLoss_personLoss3Layers_loss3layers'\n",
    "\n",
    "\n",
    "opt.checkpoints_path_task = os.path.join(opt.checkpoints_folder, opt.task_name)\n",
    "save_to_disk = get_rank() == 0\n",
    "checkpointer = DetectronCheckpointer(\n",
    "    opt, model, optimizer, scheduler, opt.checkpoints_folder, opt.checkpoints_path_task, save_to_disk, logger=logger\n",
    ")\n",
    "tid_start = 0\n",
    "epoch_start = 0\n",
    "if opt.resume != 'NoCkpt':\n",
    "    checkpoint_restored, _, _ = checkpointer.load(task_name=opt.resume)\n",
    "    if 'iteration' in checkpoint_restored:\n",
    "        tid_start = checkpoint_restored['iteration']\n",
    "    if 'epoch' in checkpoint_restored:\n",
    "        epoch_start = checkpoint_restored['epoch']\n",
    "    print(checkpoint_restored.keys())\n",
    "    logger.info(colored('Restoring from epoch %d - iter %d'%(epoch_start, tid_start), 'white', 'on_blue'))\n",
    "model.print_net()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DATASET\n",
    "train_trnfs_maskrcnn = build_transforms_maskrcnn(cfg, True)\n",
    "eval_trnfs_maskrcnn = build_transforms_maskrcnn(cfg, False)\n",
    "train_trnfs_yannick = build_transforms_yannick(cfg, True)\n",
    "eval_trnfs_yannick = build_transforms_yannick(cfg, False)\n",
    "\n",
    "ds_train_coco_vis = COCO2017Scale(\n",
    "    # transforms_yannick=train_trnfs_yannick,\n",
    "    transforms_maskrcnn=train_trnfs_maskrcnn,\n",
    "    split=\"train\",\n",
    "    shuffle=False,\n",
    "    logger=logger,\n",
    "    opt=opt,\n",
    "    coco_subset=\"coco_scale_eccv\",\n",
    ")  # !!!!!!!\n",
    "ds_eval_coco_vis = COCO2017Scale(\n",
    "    # transforms_yannick=eval_trnfs_yannick,\n",
    "    transforms_maskrcnn=eval_trnfs_maskrcnn,\n",
    "    split=\"val\",\n",
    "    shuffle=False,\n",
    "    logger=logger,\n",
    "    opt=opt,\n",
    "    coco_subset=\"coco_scale_eccv\",\n",
    ")  # !!!!!!!\n",
    "ds_test_coco_vis = COCO2017Scale(\n",
    "    # transforms_yannick=eval_trnfs_yannick,\n",
    "    transforms_maskrcnn=eval_trnfs_maskrcnn,\n",
    "    split=\"test\",\n",
    "    shuffle=False,\n",
    "    logger=logger,\n",
    "    opt=opt,\n",
    "    coco_subset=\"coco_scale_eccv\",\n",
    ")  # !!!!!!!\n",
    "\n",
    "training_loader_coco_vis = make_data_loader(\n",
    "    cfg,\n",
    "    ds_train_coco_vis,\n",
    "    is_train=False,\n",
    "    is_distributed=False,\n",
    "    start_iter=0,\n",
    "    logger=logger,\n",
    "    collate_fn=my_collate,\n",
    "    batch_size_override=1,  # BN does not make sense when model.train() and batchsize==1!\n",
    ")\n",
    "eval_loader_coco_vis = make_data_loader(\n",
    "    cfg,\n",
    "    ds_eval_coco_vis,\n",
    "    is_train=False,\n",
    "    is_distributed=False,\n",
    "    is_for_period=True,\n",
    "    logger=logger,\n",
    "    collate_fn=my_collate,\n",
    "    batch_size_override=1,\n",
    ")\n",
    "test_loader_coco_vis = make_data_loader(\n",
    "    cfg,\n",
    "    ds_test_coco_vis,\n",
    "    is_train=False,\n",
    "    is_distributed=False,\n",
    "    is_for_period=True,\n",
    "    logger=logger,\n",
    "    collate_fn=my_collate,\n",
    "    batch_size_override=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval-RELEASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = 'release_results'\n",
    "from pathlib import Path\n",
    "Path(results_path).mkdir(exist_ok=True)\n",
    "task_name = opt.resume\n",
    "task_name_appendix = '-predH'\n",
    "# task_name_appendix = '-fitH'\n",
    "task_name += task_name_appendix\n",
    "\n",
    "# task_name = 'tmp'\n",
    "print(task_name)\n",
    "\n",
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "write_folder = os.path.join(results_path, task_name)\n",
    "for subfolder in ['', 'png', 'npy', 'pickle', 'results']:\n",
    "    Path(os.path.join(write_folder, subfolder)).mkdir(parents=True, exist_ok=True)\n",
    "results_path_png = os.path.join(write_folder, 'png')\n",
    "results_path_results = os.path.join(write_folder, 'results')\n",
    "\n",
    "is_training = False\n",
    "if_vis = True\n",
    "if_debug = False\n",
    "prepostfix='testSet-|-evalMode'\n",
    "\n",
    "test_loader = training_loader_coco_vis\n",
    "# test_loader = eval_loader_coco_vis\n",
    "# test_loader = test_loader_coco_vis\n",
    "\n",
    "if_blender = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed as dist\n",
    "import numpy as np\n",
    "opt.zero_pitch = False\n",
    "\n",
    "# select_show = -1\n",
    "select_show = 0\n",
    "# select_show = 35, 38, 41 # large pitch\n",
    "# select_show = 35\n",
    "# select_show = 923 # 30 degree pitch\n",
    "# select_show = 12\n",
    "# select_show = 18\n",
    "# select_show = 15 # example of fitting person height\n",
    "\n",
    "from utils.model_utils import get_bins_combine\n",
    "import utils.model_utils as model_utils\n",
    "\n",
    "from utils.utils_misc import batch_dict_to_list_of_dicts\n",
    "from utils import utils_coco\n",
    "from utils.train_utils import f_pixels_to_mm\n",
    "import utils.geo_utils as geo_utils\n",
    "\n",
    "import skimage.io as io\n",
    "import utils.utils_coco as utils_coco\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "logger = setup_logger(\"logger:train\", summary_path, get_rank(), filename=\"logger_maskrcn-style.txt\")\n",
    "\n",
    "## START TRAINING\n",
    "best_loss = float('inf')\n",
    "bins = get_bins_combine(device)\n",
    "tid = 0\n",
    "\n",
    "epoch = 0\n",
    "eval_loss = 0\n",
    "\n",
    "model.eval()\n",
    "\n",
    "cpu_device = torch.device(\"cpu\")\n",
    "loss_func = torch.nn.L1Loss()\n",
    "if opt.distributed:\n",
    "    rank = dist.get_rank()\n",
    "else:\n",
    "    rank = 0\n",
    "    \n",
    "eval_loss_vt_list = []\n",
    "eval_loss_person_list = []\n",
    "vt_loss_allBoxes_dict = {}\n",
    "vt_loss_allBoxes_dict_list = []\n",
    "\n",
    "im_filename_list = []\n",
    "\n",
    "test_list = []\n",
    "\n",
    "vt_loss_all =[]\n",
    "\n",
    "num_plots = 0\n",
    "pitch_abs_list = []\n",
    "with torch.no_grad():\n",
    "    for i, (inputCOCO_Image_maskrcnnTransform_list, W_batch_array, H_batch_array, yc_batch, \\\n",
    "            bboxes_batch_array, bboxes_length_batch_array, v0_batch, f_pixels_yannick_batch, im_filename, im_file, target_maskrcnnTransform_list, labels_list, _) in tqdm(enumerate(test_loader)):\n",
    "        \n",
    "        if select_show != -1 and i < select_show:\n",
    "            continue\n",
    "        if_vis = True\n",
    "        if_blender = True\n",
    "        tid = i\n",
    "\n",
    "        input_dict = {'inputCOCO_Image_maskrcnnTransform_list': inputCOCO_Image_maskrcnnTransform_list, 'W_batch_array': W_batch_array, 'H_batch_array': H_batch_array, \\\n",
    "                      'yc_batch': yc_batch, \\\n",
    "                      'bboxes_batch_array': bboxes_batch_array, 'bboxes_length_batch_array': bboxes_length_batch_array, \\\n",
    "                      'v0_batch': v0_batch, 'f_pixels_yannick_batch': f_pixels_yannick_batch, 'im_filename': im_filename, 'im_file': im_file, \\\n",
    "                      'bins': bins, 'target_maskrcnnTransform_list': target_maskrcnnTransform_list, 'labels_list': labels_list}\n",
    "        if_print = is_training\n",
    "        cfg = opt.cfg\n",
    "        bins = input_dict['bins']\n",
    "\n",
    "        # ========= Rui's inputs\n",
    "        inputCOCO_Image_maskrcnnTransform_list = input_dict['inputCOCO_Image_maskrcnnTransform_list']\n",
    "        bboxes_batch, v0_batch_offline, f_pixels_yannick_batch_offline, W_batch, H_batch, yc_batch_offline = \\\n",
    "            torch.from_numpy(input_dict['bboxes_batch_array']).float().to(device), input_dict['v0_batch'].to(device), input_dict['f_pixels_yannick_batch'][0].to(device), \\\n",
    "            torch.from_numpy(input_dict['W_batch_array']).to(device), torch.from_numpy(input_dict['H_batch_array']).to(device), input_dict['yc_batch'].to(device)\n",
    "\n",
    "        if not opt.not_rcnn:\n",
    "            list_of_bbox_list_cpu = model_utils.bboxArray_to_bboxList(bboxes_batch, input_dict['bboxes_length_batch_array'], input_dict['W_batch_array'], input_dict['H_batch_array'])\n",
    "        else:\n",
    "            list_of_bbox_list_cpu = []\n",
    "\n",
    "        list_of_oneLargeBbox_list_cpu = model_utils.oneLargeBboxList(input_dict['W_batch_array'], input_dict['H_batch_array']) # in original image size\n",
    "        list_of_oneLargeBbox_list = [bbox_list_array.to(device) for bbox_list_array in list_of_oneLargeBbox_list_cpu]\n",
    "\n",
    "\n",
    "        if if_vis:\n",
    "            input_dict_show = {'H': input_dict['H_batch_array'], 'W': input_dict['W_batch_array']}\n",
    "            if 'testSet' not in prepostfix:\n",
    "                input_dict_show.update({'v0_cocoPredict': input_dict['v0_batch'].numpy()})\n",
    "            input_dict_show['bbox_gt'] = []\n",
    "            input_dict_show['bbox_est'] = []\n",
    "            input_dict_show['bbox_fit'] = []\n",
    "            input_dict_show['bbox_h'] = []\n",
    "            input_dict_show['bbox_geo'] = []\n",
    "            input_dict_show['bbox_loss'] = []\n",
    "\n",
    "        input_dict_misc = {'bins': bins, 'is_training': is_training, 'H_batch': H_batch, 'W_batch': W_batch, 'bboxes_batch': bboxes_batch, 'loss_func': loss_func, \\\n",
    "                           'cpu_device': cpu_device,  'device': device, 'tid': tid, 'rank': rank, 'data': 'coco', 'if_vis': if_vis}\n",
    "        output_RCNN = model(input_dict_misc=input_dict_misc, input_dict=input_dict, image_batch_list=inputCOCO_Image_maskrcnnTransform_list, \\\n",
    "                            list_of_bbox_list_cpu=list_of_bbox_list_cpu, list_of_oneLargeBbox_list=list_of_oneLargeBbox_list, \\\n",
    "                            )\n",
    "        pitch_abs_degree = abs(output_RCNN['pitch_batch_est'][0].item()) / np.pi * 180.\n",
    "        pitch_abs_list.append(pitch_abs_degree)\n",
    "        if 'zero_pitch' in opt and opt.zero_pitch:\n",
    "            output_RCNN['pitch_batch_est'] *= 0.\n",
    "            output_RCNN['output_pitch'] *= 0.\n",
    "            output_RCNN['v0_batch_from_pitch_vfov'] = output_RCNN['v0_batch_from_pitch_vfov'] * 0. + H_batch[0] / 2.\n",
    "            output_RCNN['v0_batch_est'] = output_RCNN['v0_batch_from_pitch_vfov']\n",
    "            \n",
    "        output_horizon = output_RCNN['output_horizon']\n",
    "        output_pitch = output_RCNN['output_pitch']\n",
    "        output_vfov = output_RCNN['output_vfov']\n",
    "        output_yc_batch = output_RCNN['output_yc_batch']\n",
    "        f_pixels_yannick_batch_ori = f_pixels_yannick_batch_offline.clone()\n",
    "        v0_batch_ori = v0_batch_offline.clone()\n",
    "        \n",
    "        v0_batch_est = output_RCNN['v0_batch_est']\n",
    "        v0_batch_from_pitch_vfov = output_RCNN['v0_batch_from_pitch_vfov']\n",
    "\n",
    "\n",
    "        vfov_estim = output_RCNN['vfov_estim']\n",
    "        f_pixels_yannick_batch_est = output_RCNN['f_pixels_batch_est']\n",
    "\n",
    "        pitch_batch_est = output_RCNN['pitch_batch_est']\n",
    "        pitch_estim_yannick = output_RCNN['pitch_estim_yannick']\n",
    "\n",
    "        yc_est_batch = output_RCNN['yc_est_batch']\n",
    "\n",
    "        person_h_list = output_RCNN['person_h_list']\n",
    "        all_person_hs = output_RCNN['all_person_hs']\n",
    "\n",
    "        reduce_method = output_RCNN['reduce_method']\n",
    "        \n",
    "\n",
    "\n",
    "        if tid % opt.summary_every_iter == 0 and if_print:\n",
    "            f_mm_array_est = f_pixels_to_mm(f_pixels_yannick_batch_est, input_dict)\n",
    "\n",
    "        ## Losses for bbox fitting\n",
    "        vt_loss_sample_batch_list = []\n",
    "        vt_loss_allBoxes_dict_cpu = {}\n",
    "        vt_error_fit_allBoxes_dict_cpu = {}\n",
    "        camH_fit_batch = []\n",
    "\n",
    "        for idx, bboxes_length in enumerate(input_dict['bboxes_length_batch_array']):\n",
    "            bboxes = bboxes_batch[idx][:bboxes_length] # [N, 4]\n",
    "            # W = W_batch[idx]\n",
    "            H = H_batch[idx]\n",
    "            vc = H / 2.\n",
    "            v0_est = v0_batch_est[idx]\n",
    "            v0_ori = v0_batch_ori[idx]  # [top H, bottom 0]\n",
    "            pitch_est = pitch_batch_est[idx]\n",
    "            f_pixels_yannick_est = f_pixels_yannick_batch_est[idx]\n",
    "            inv_f2 = 1. / (f_pixels_yannick_est * f_pixels_yannick_est)\n",
    "            yc_est = yc_est_batch[idx]\n",
    "\n",
    "            H_np = H_batch[idx].cpu().numpy()\n",
    "            W_np = W_batch[idx].cpu().numpy()\n",
    "\n",
    "            if opt.not_rcnn:\n",
    "                h_human_s = torch.from_numpy(np.asarray([1.75] * bboxes.shape[0], dtype=np.float32)).float().to(device)\n",
    "            else:\n",
    "                h_human_s = person_h_list[idx] * output_RCNN['straighten_ratios_list'][idx]\n",
    "\n",
    "            vt_loss_sample_list = []\n",
    "            vt_error_sample_fit_list = []\n",
    "\n",
    "            camH_fit_list = []\n",
    "            for bbox_idx, bbox in enumerate(bboxes):\n",
    "                y_person_fit = 1.75\n",
    "                camH_fit_bbox = geo_utils.fit_camH(bbox.cpu(), H.cpu(), v0_est.cpu(), vc.cpu(), f_pixels_yannick_est.cpu(), y_person_fit)\n",
    "                camH_fit_list.append(camH_fit_bbox.detach().numpy())\n",
    "            camH_fit = np.median(np.array(camH_fit_list))\n",
    "            camH_fit_batch.append(camH_fit)\n",
    "\n",
    "            bbox_gt_sample = []\n",
    "            bbox_est_sample = []\n",
    "            bbox_fit_sample = []\n",
    "            bbox_h_sample = []\n",
    "            bbox_geo_sample = []\n",
    "            bbox_loss_sample = []\n",
    "            for bbox_idx, (bbox, y_person) in enumerate(zip(bboxes, h_human_s)):\n",
    "                vb = H - (bbox[1] + bbox[3]) # [top H bottom 0]\n",
    "                vt_gt = H - bbox[1] # [top H bottom 0]\n",
    "                \n",
    "                ## Fit y_person\n",
    "                geo_model_input_dict = {'yc_est': yc_est, 'vb': vb, 'y_person': y_person, 'v0': v0_est, 'vc': vc, 'f_pixels_yannick': f_pixels_yannick_est, 'pitch_est': pitch_est}\n",
    "                negative_z = False\n",
    "                if opt.accu_model:\n",
    "                    vt_camEst, z, negative_z = model_utils.accu_model_helanyi(geo_model_input_dict, if_debug=False)  # [top H bottom 0]\n",
    "                    print('======>>>>>Helanyi:', vt_camEst.item())\n",
    "                    geo_model_input_dict_rui = {'yc_est': yc_est, 'vb': vb, 'y_person': y_person*torch.cos(pitch_est), 'v0': v0_est, 'vc': vc, 'f_pixels_yannick': f_pixels_yannick_est, 'pitch_est': pitch_est}\n",
    "                    vt_camEst_rui, z_rui, negative_z_rui = model_utils.accu_model(geo_model_input_dict_rui, if_debug=False)  # [top H bottom 0]\n",
    "                    print('======>>>>>RUI----:', vt_camEst_rui.item())\n",
    "#                     geo_model_input_dict_rui = {'yc_est': yc_est, 'vb': vb, 'y_person': y_person, 'v0': v0_est, 'vc': vc, 'f_pixels_yannick': f_pixels_yannick_est, 'pitch_est': pitch_est}\n",
    "#                     vt_camEst_rui, z_rui, negative_z_rui = model_utils.accu_model_fixedz(geo_model_input_dict_rui, if_debug=False)  # [top H bottom 0]\n",
    "#                     print('======>>>>>RUI FIXEDZ----:', vt_camEst_rui.item())\n",
    "                else:\n",
    "                    vt_camEst = model_utils.approx_model(geo_model_input_dict)\n",
    "\n",
    "                vt_loss = loss_func(vt_gt, vt_camEst.reshape([])) / bbox[3]\n",
    "                vt_loss = torch.clamp(vt_loss, 0., opt.cfg.MODEL.LOSS.VT_LOSS_CLAMP)\n",
    "                if negative_z and if_debug:\n",
    "                    print('>>>>>>', negative_z, vt_loss)\n",
    "                # vt_loss = vt_loss * (not(negative_z))\n",
    "                vt_loss = torch.where(torch.isnan(vt_loss), torch.zeros_like(vt_loss), vt_loss)\n",
    "                if negative_z and if_debug:\n",
    "                    print('>>>>>>>>>>>', vt_loss)\n",
    "                vt_loss_sample_list.append(vt_loss)\n",
    "                vt_loss_allBoxes_dict_cpu.update({'bbox_vt_loss_tid%04d_rank%d_%02d-%02d'%(tid, rank, idx, bbox_idx): vt_loss.to(cpu_device)})\n",
    "\n",
    "                # Fitting 现场: getting vt\n",
    "                y_person_fit = 1.75\n",
    "                vt_camFit = geo_utils.fit_vt(camH_fit, vb, v0_est, vc, y_person_fit, 1. / (f_pixels_yannick_est  * f_pixels_yannick_est))\n",
    "                vt_error_fit = loss_func(vt_gt, vt_camFit) / bbox[3]\n",
    "\n",
    "                vt_error_fit_allBoxes_dict_cpu.update({'bbox_vt_error_fit_tid%04d_rank%d_%02d-%02d'%(tid, rank, idx, bbox_idx): vt_error_fit.to(cpu_device)})\n",
    "\n",
    "                vt_error_sample_fit_list.append(vt_error_fit.detach().cpu().numpy().item())\n",
    "\n",
    "                if if_vis:\n",
    "                    bbox_np = bbox.cpu().numpy()\n",
    "                    vt_camEst_np = vt_camEst.detach().cpu().numpy()\n",
    "                    vt_camFit_np = vt_camFit.detach().cpu().numpy()\n",
    "                    bbox_gt_sample.append([bbox_np[0], bbox_np[1], bbox_np[2], bbox_np[3]]) # [x, y (top), w, h]\n",
    "                    bbox_est_sample.append([bbox_np[0], H_np - vt_camEst_np, bbox_np[2], bbox_np[1]+bbox_np[3]-(H_np - vt_camEst_np)])\n",
    "                    bbox_h_sample.append(y_person.cpu().detach().numpy())\n",
    "                    bbox_geo_sample.append(geo_model_input_dict)\n",
    "                    bbox_loss_sample.append(vt_loss.item())\n",
    "\n",
    "            if if_vis:\n",
    "                input_dict_show['bbox_gt'].append(bbox_gt_sample)\n",
    "                input_dict_show['bbox_est'].append(bbox_est_sample)\n",
    "                input_dict_show['bbox_fit'].append(bbox_fit_sample)\n",
    "                input_dict_show['bbox_h'].append(bbox_h_sample)\n",
    "                input_dict_show['bbox_geo'].append(bbox_geo_sample)\n",
    "                input_dict_show['bbox_loss'].append(bbox_loss_sample)\n",
    "\n",
    "\n",
    "            vt_loss_sample = torch.mean(torch.stack(vt_loss_sample_list))\n",
    "            vt_loss_sample_batch_list.append(vt_loss_sample)\n",
    "\n",
    "        vt_loss_batch = torch.stack(vt_loss_sample_batch_list)\n",
    "        loss_vt = torch.mean(vt_loss_batch)\n",
    "\n",
    "        return_dict = {'vt_loss_batch': vt_loss_batch, 'vt_loss_allBoxes_dict': vt_loss_allBoxes_dict_cpu, 'vt_error_fit_allBoxes_dict': vt_error_fit_allBoxes_dict_cpu, \\\n",
    "                       'yc_est_batch': yc_est_batch, 'yc_batch_offline': yc_batch_offline, 'yc_fit_batch': np.array(camH_fit_batch)}\n",
    "        vt_loss_allBoxes_dict_list.append(vt_loss_allBoxes_dict_cpu)\n",
    "        if tid % opt.summary_every_iter == 0 and if_print:\n",
    "            return_dict.update({'f_mm_batch': f_mm_array_est.reshape(-1, 1)})\n",
    "\n",
    "        if 'loss_vt_list' in output_RCNN:\n",
    "            loss_vt_list = output_RCNN['loss_vt_list']\n",
    "            assert len(loss_vt_list) != 0\n",
    "        else:\n",
    "            loss_vt_list = []\n",
    "        loss_vt_list.append(loss_vt)\n",
    "        return_dict.update({'loss_vt_list': loss_vt_list})\n",
    "        loss_dict = {'loss_vt': sum(loss_vt_list)/len(loss_vt_list)} # mean of layers; for optimization and scheduler\n",
    "\n",
    "        if opt.pointnet_camH_refine:\n",
    "            loss_vt_layers_dict = {}\n",
    "            for loss_idx, loss in enumerate(loss_vt_list):\n",
    "                loss_vt_layers_dict['loss_vt_layer_%d'%(loss_idx-len(loss_vt_list))] = loss\n",
    "\n",
    "            return_dict.update({'loss_vt_layers_dict': loss_vt_layers_dict})\n",
    "\n",
    "\n",
    "        if not opt.not_rcnn:\n",
    "            return_dict.update({'all_person_hs': all_person_hs})\n",
    "            loss_all_person_h_list = output_RCNN['loss_all_person_h_list']\n",
    "            return_dict.update({'loss_all_person_h_list': loss_all_person_h_list})\n",
    "            loss_dict.update({'loss_person': sum(loss_all_person_h_list)/len(loss_all_person_h_list)}) # mean of layers; for optimization and scheduler\n",
    "\n",
    "\n",
    "        # ========== Some vis\n",
    "        input_dict_show['W_batch_array'] = input_dict['W_batch_array']\n",
    "        input_dict_show['H_batch_array'] = input_dict['H_batch_array']\n",
    "        if if_vis:\n",
    "            if opt.est_kps:    \n",
    "                input_dict_show['predictions'] = output_RCNN['predictions']\n",
    "                input_dict_show['target_maskrcnnTransform_list'] = input_dict['target_maskrcnnTransform_list']\n",
    "#             input_dict_show['v0_batch_predict'] = v0_batch_predict.detach().cpu().numpy()  # (H = top of the image, 0 = bottom of the image)\n",
    "            input_dict_show['v0_batch_from_pitch_vfov'] = v0_batch_from_pitch_vfov.detach().cpu().numpy()\n",
    "            input_dict_show['v0_batch_est'] = v0_batch_est.detach().cpu().numpy()\n",
    "            if 'v0_batch_est_0' in output_RCNN:\n",
    "                input_dict_show['v0_batch_est_0'] = output_RCNN['v0_batch_est_0'].detach().cpu().numpy()\n",
    "            # f_pixels_yannick_single_est = f_pixels_yannick_batch_est.detach().cpu().numpy()\n",
    "            # f_pixels_yannick_single_est_mm = [utils_coco.fpix_to_fmm(f_pixels_yannick_single_est_0, H_np, W_np) for f_pixels_yannick_single_est_0 in f_pixels_yannick_single_est]\n",
    "            # f_pixels_yannick_single_ori = f_pixels_yannick_batch_ori.detach().cpu().numpy()\n",
    "            # f_pixels_yannick_single_ori_mm = [utils_coco.fpix_to_fmm(f_pixels_yannick_single_ori_0, H_np, W_np) for f_pixels_yannick_single_ori_0 in f_pixels_yannick_single_ori]\n",
    "            input_dict_show.update({'yc_fit': yc_batch_offline.detach().cpu().numpy(), 'yc_est': yc_est_batch.detach().cpu().numpy()})\n",
    "\n",
    "            if len(output_RCNN['yc_est_batch_np_list']) > 1: # more than one layers\n",
    "                input_dict_show.update({'yc_est_list': output_RCNN['yc_est_batch_np_list']})\n",
    "            if len(output_RCNN['person_hs_est_np_list']) > 1:\n",
    "                input_dict_show.update({'person_hs_est_list': output_RCNN['person_hs_est_np_list']})\n",
    "            if 'vt_camEst_N_delta_np_list' in output_RCNN and len(output_RCNN['vt_camEst_N_delta_np_list']) > 1:\n",
    "                input_dict_show.update({'vt_camEst_N_delta_est_list': output_RCNN['vt_camEst_N_delta_np_list']})\n",
    "            if len(output_RCNN['f_pixels_est_batch_np_list']) > 1: # more than one layers\n",
    "                input_dict_show.update({'f_pixels_est_mm_list': [utils_coco.fpix_to_fmm(f_pixels_est_0, H_np, W_np) for f_pixels_est_0 in output_RCNN['f_pixels_est_batch_np_list']]})\n",
    "            if len(output_RCNN['v0_01_est_batch_np_list']) > 1: # more than one layers\n",
    "                input_dict_show.update({'v0_est_list': output_RCNN['v0_01_est_batch_np_list']})\n",
    "\n",
    "            # input_dict_show.update({'f_est_px': f_pixels_yannick_single_est, 'f_est_mm': f_pixels_yannick_single_est_mm})\n",
    "            # input_dict_show.update({'f_cocoPredict': f_pixels_yannick_single_ori, 'f_cocoPredict_mm': f_pixels_yannick_single_ori_mm})\n",
    "            input_dict_show.update({'pitch_est_angle': pitch_batch_est.detach().cpu().numpy()/np.pi*180.})\n",
    "\n",
    "            input_dict_show['im_path'] = list(input_dict['im_file'])\n",
    "            input_dict_show['im_filename'] = list(input_dict['im_filename'])\n",
    "            num_samples = len(input_dict['im_file'])\n",
    "            input_dict_show['output_horizon_COCO'] = output_RCNN['output_horizon'].detach().cpu().numpy()\n",
    "            input_dict_show['horizon_bins'] = [bins['horizon_bins_centers_torch'].cpu().numpy()] * num_samples\n",
    "\n",
    "            if not opt.direct_camH:\n",
    "                input_dict_show['output_camH_COCO'] = output_RCNN['output_yc_batch'].detach().cpu().numpy()\n",
    "                input_dict_show['camH_bins'] = [bins['yc_bins_centers_torch'].cpu().numpy()] * num_samples\n",
    "\n",
    "            input_dict_show['tid'] = [tid] * num_samples\n",
    "            input_dict_show['task_name'] = [opt.task_name] * num_samples\n",
    "            input_dict_show['num_samples'] = num_samples\n",
    "            input_dict_show['reduce_method'] = [reduce_method] * num_samples\n",
    "            input_dict_show.update({'vfov_est': vfov_estim.detach().cpu().numpy(), 'pitch_est_yannick': pitch_estim_yannick.detach().cpu().numpy()})\n",
    "\n",
    "            if input_dict_show['num_samples'] > 0:\n",
    "                input_dict_show_list = batch_dict_to_list_of_dicts(input_dict_show)\n",
    "                input_dict_show = input_dict_show_list[0]\n",
    "\n",
    "        prefix, postfix = prepostfix.split('|')\n",
    "        \n",
    "        save_path_blender = results_path_png\n",
    "        save_path_ours = results_path_png\n",
    "        prefix = 'coco_' + prefix\n",
    "        postfix += '_reproj'\n",
    "        \n",
    "        if if_vis:\n",
    "            vis_utils.show_cam_bbox(io.imread(input_dict_show['im_path']), input_dict_show, figzoom=1.5, if_show=True, \\\n",
    "                                    if_save=True if save_path_ours else False, if_pause=False, save_path=save_path_ours, save_name=prefix+'%06d'%(tid)+postfix+'_reproj')\n",
    "            if opt.est_kps:\n",
    "                vis_utils.show_box_kps(opt, model, io.imread(input_dict_show['im_path']), input_dict_show, if_show=True, if_pause=False, save_path='', save_name=prefix+'tid%d'%(tid)+postfix)\n",
    "            \n",
    "            if if_blender:\n",
    "                tmp_code = ''.join([random.choice(string.ascii_letters + string.digits) for n in range(32)])\n",
    "                vis_utils.blender_render(input_dict_show, output_RCNN, im_file, tmp_code=tmp_code, render_type='chair', pick=-1, grid=False, current_dir=os.path.join(os.getcwd()), save_name=tmp_code)\n",
    "        num_plots += 1\n",
    "        if num_plots >=1:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

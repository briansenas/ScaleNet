{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "\n",
    "import os, sys, inspect\n",
    "from pathlib import Path\n",
    "\n",
    "current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "print(current_dir)\n",
    "sys.path.insert(0, current_dir)\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "torch_version = torch.__version__\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from models.model_RCNNOnly_combine_indeptPointnet_maskrcnnPose_discount import (\n",
    "    RCNNOnly_combine,\n",
    ")\n",
    "from dataset_coco_pickle_eccv import my_collate, COCO2017ECCV\n",
    "from utils.data_utils import make_data_loader\n",
    "from utils.utils_misc import colored\n",
    "from maskrcnn_rui.data.transforms import (\n",
    "    build_transforms_maskrcnn,\n",
    "    build_transforms_yannick,\n",
    ")\n",
    "import torch.distributed as dist\n",
    "import numpy as np\n",
    "from utils.model_utils import get_bins_combine\n",
    "import utils.model_utils as model_utils\n",
    "from utils.train_utils import f_pixels_to_mm\n",
    "import utils.geo_utils as geo_utils\n",
    "from tqdm import tqdm\n",
    "from utils.logger import setup_logger, printer\n",
    "\n",
    "from maskrcnn_rui.config import cfg\n",
    "from maskrcnn_rui.utils.comm import get_rank\n",
    "from utils.checkpointer import DetectronCheckpointer\n",
    "import random\n",
    "import numpy as np\n",
    "from utils.model_utils import get_bins_combine\n",
    "import utils.model_utils as model_utils\n",
    "\n",
    "from utils.utils_misc import batch_dict_to_list_of_dicts\n",
    "from utils.train_utils import f_pixels_to_mm\n",
    "import utils.geo_utils as geo_utils\n",
    "\n",
    "import skimage.io as io\n",
    "from utils.utils_coco import fpix_to_fmm\n",
    "from utils.vis_utils import blender_render, show_cam_bbox, show_box_kps\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from utils.eval_save_utils_combine_RCNNONly import check_eval_COCO, check_save\n",
    "from utils.utils_misc import green\n",
    "\n",
    "\n",
    "seed = 140421\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Rui's Scale Estimation Network Training\")\n",
    "# Training\n",
    "parser.add_argument(\"--task_name\", type=str, default=\"tmp\", help=\"resume training\")\n",
    "parser.add_argument(\n",
    "    \"--workers\", type=int, help=\"number of data loading workers\", default=8\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--save_every_iter\",\n",
    "    type=int,\n",
    "    default=0,\n",
    "    help=\"set to 0 to save ONLY at the end of each epoch\",\n",
    ")\n",
    "parser.add_argument(\"--summary_every_iter\", type=int, default=100, help=\"\")\n",
    "parser.add_argument(\n",
    "    \"--nepoch\", type=int, default=3, help=\"number of epochs to train for\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--beta1\", type=float, default=0.9, help=\"beta1 for adam. default=0.5\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--not_val\", action=\"store_true\", help=\"Do not validate duruign training\"\n",
    ")\n",
    "parser.add_argument(\"--not_vis\", action=\"store_true\", help=\"\")\n",
    "parser.add_argument(\"--not_vis_SUN360\", action=\"store_true\", help=\"\")\n",
    "parser.add_argument(\n",
    "    \"--save_every_epoch\", type=int, default=1, help=\"save checkpoint every ? epoch\"\n",
    ")\n",
    "parser.add_argument(\"--vis_every_epoch\", type=int, default=5, help=\"vis every ? epoch\")\n",
    "# Model\n",
    "parser.add_argument(\n",
    "    \"--accu_model\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Use accurate model with theta instead of Derek's approx.\",\n",
    ")\n",
    "parser.add_argument(\"--argmax_val\", action=\"store_true\", help=\"\")\n",
    "parser.add_argument(\n",
    "    \"--direct_camH\",\n",
    "    action=\"store_true\",\n",
    "    help=\"direct preidict one number for camera height ONLY, instead of predicting a distribution\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--direct_v0\",\n",
    "    action=\"store_true\",\n",
    "    help=\"direct preidict one number for v0 ONLY, instead of predicting a distribution\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--direct_fmm\",\n",
    "    action=\"store_true\",\n",
    "    help=\"direct preidict one number for fmm ONLY, instead of predicting a distribution\",\n",
    ")\n",
    "\n",
    "# Pre-training\n",
    "parser.add_argument(\n",
    "    \"--resume\",\n",
    "    type=str,\n",
    "    help=\"resume training; can be full path (e.g. tmp/checkpoint0.pth.tar) or taskname (e.g. tmp)\",\n",
    "    default=\"NoCkpt\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--feature_only\",\n",
    "    action=\"store_true\",\n",
    "    help=\"restore only features (remove all classifiers) from checkpoint\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--reset_scheduler\", action=\"store_true\", help=\"\"\n",
    ")  # NOT working yet\n",
    "parser.add_argument(\"--reset_lr\", action=\"store_true\", help=\"\")  # NOT working yet\n",
    "\n",
    "# Device\n",
    "parser.add_argument(\"--cpu\", action=\"store_true\", help=\"Force training on CPU\")\n",
    "parser.add_argument(\"--local_rank\", type=int, default=0)\n",
    "parser.add_argument(\"--master_port\", type=str, default=\"8914\")\n",
    "\n",
    "# DEBUG\n",
    "parser.add_argument(\"--debug\", action=\"store_true\", help=\"Debug eval\")\n",
    "parser.add_argument(\"--debug_memory\", action=\"store_true\", help=\"Debug eval\")\n",
    "\n",
    "# Mask R-CNN\n",
    "## Modules\n",
    "parser.add_argument(\n",
    "    \"--train_cameraCls\", action=\"store_true\", help=\"Disable camera calibration network\"\n",
    ")\n",
    "parser.add_argument(\"--train_roi_h\", action=\"store_true\", help=\"\")\n",
    "parser.add_argument(\n",
    "    \"--est_bbox\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Enable estimating bboxes instead of using GT bboxes\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--est_kps\", action=\"store_true\", help=\"Enable estimating keypoints\"\n",
    ")\n",
    "parser.add_argument(\"--if_discount\", action=\"store_true\", help=\"\")\n",
    "parser.add_argument(\"--discount_from\", type=str, default=\"pred\")  # ('GT', 'pred')\n",
    "\n",
    "## Losses\n",
    "parser.add_argument(\n",
    "    \"--loss_last_layer\", action=\"store_true\", help=\"Using loss of last layer only\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--loss_person_all_layers\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Using loss of last layer only\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--not_rcnn\", action=\"store_true\", help=\"Disable Mask R-CNN person height bbox head\"\n",
    ")\n",
    "parser.add_argument(\"--no_kps_loss\", action=\"store_true\", help=\"\")\n",
    "\n",
    "## Archs\n",
    "parser.add_argument(\"--pointnet_camH\", action=\"store_true\", help=\"\")\n",
    "parser.add_argument(\"--pointnet_camH_refine\", action=\"store_true\", help=\"\")\n",
    "parser.add_argument(\"--pointnet_personH_refine\", action=\"store_true\", help=\"\")\n",
    "parser.add_argument(\n",
    "    \"--pointnet_roi_feat_input\", action=\"store_true\", help=\"\"\n",
    ")  # NOT working yet\n",
    "parser.add_argument(\n",
    "    \"--pointnet_roi_feat_input_person3\", action=\"store_true\", help=\"\"\n",
    ")  # NOT working yet\n",
    "parser.add_argument(\n",
    "    \"--pointnet_fmm_refine\", action=\"store_true\", help=\"\"\n",
    ")  # NOT working yet\n",
    "parser.add_argument(\n",
    "    \"--pointnet_v0_refine\", action=\"store_true\", help=\"\"\n",
    ")  # NOT working yet\n",
    "parser.add_argument(\n",
    "    \"--not_pointnet_detach_input\", action=\"store_true\", help=\"\"\n",
    ")  # NOT working yet\n",
    "parser.add_argument(\"--num_layers\", type=int, default=3)\n",
    "parser.add_argument(\"--fit_derek\", action=\"store_true\", help=\"\")\n",
    "## weights\n",
    "parser.add_argument(\n",
    "    \"--weight_SUN360\",\n",
    "    type=float,\n",
    "    default=10.0,\n",
    "    help=\"weight for Yannick's losses. default=1.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--weight_kps\",\n",
    "    type=float,\n",
    "    default=1e-3,\n",
    "    help=\"weight for Yannick's losses. default=1.\",\n",
    ")\n",
    "\n",
    "## debug\n",
    "parser.add_argument(\"--zero_pitch\", action=\"store_true\", help=\"\")  # NOT working yet\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--config-file\",\n",
    "    default=\"\",\n",
    "    metavar=\"FILE\",\n",
    "    help=\"path to config file\",\n",
    "    type=str,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"opts\",\n",
    "    help=\"Modify config options using the command-line\",\n",
    "    default=None,\n",
    "    nargs=argparse.REMAINDER,\n",
    ")\n",
    "\n",
    "opt = parser.parse_args(\n",
    "    (\"--task_name tmp_eval --num_layers 3 --train_cameraCls --train_roi_h --pointnet_camH \" + \n",
    "    \"--pointnet_camH_refine --pointnet_personH_refine --accu_model --est_kps --est_bbox \" + \n",
    "    \"--loss_person_all_layers --config-file config/coco_config_small_synBN1108_kps.yaml  \" + \n",
    "    \"--weight_SUN360 10. \"\n",
    "    ).split()\n",
    ")\n",
    "\n",
    "print(opt)\n",
    "opt.debug = True\n",
    "opt.checkpoints_folder = \"checkpoint\"\n",
    "\n",
    "config_file = opt.config_file\n",
    "cfg.merge_from_file(config_file)\n",
    "# manual override some options\n",
    "cfg.merge_from_list([\"MODEL.DEVICE\", \"cuda\"])\n",
    "cfg.merge_from_list(opt.opts)\n",
    "cfg.freeze()\n",
    "\n",
    "opt.cfg = cfg\n",
    "\n",
    "# sys.path.insert(0, cfg.MODEL.POINTNET.PATH)\n",
    "opt.rank = opt.local_rank\n",
    "\n",
    "num_gpus = 1\n",
    "opt.distributed = num_gpus > 1\n",
    "device = \"cuda\"\n",
    "opt.device = device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SUMMARY WRITERS\n",
    "summary_path = \"./summary/\" + opt.task_name\n",
    "writer = SummaryWriter(summary_path)\n",
    "\n",
    "# === LOGGING\n",
    "logger = setup_logger(\n",
    "    \"logger:train\", summary_path, get_rank(), filename=\"logger_maskrcn-style.txt\"\n",
    ")\n",
    "logger.info(colored(\"==[config]== opt\", \"white\", \"on_blue\"))\n",
    "logger.info(opt)\n",
    "logger.info(colored(\"==[config]== cfg\", \"white\", \"on_blue\"))\n",
    "logger.info(cfg)\n",
    "logger.info(\n",
    "    colored(\n",
    "        \"==[config]== Loaded configuration file {}\".format(opt.config_file),\n",
    "        \"white\",\n",
    "        \"on_blue\",\n",
    "    )\n",
    ")\n",
    "with open(opt.config_file, \"r\") as cf:\n",
    "    config_str = \"\\n\" + cf.read()\n",
    "    logger.info(config_str)\n",
    "printer = printer(get_rank(), debug=opt.debug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MODEL & TRAINING\n",
    "modules_not_build = []\n",
    "if not opt.train_cameraCls:\n",
    "    modules_not_build.append(\"classifier_heads\")\n",
    "if not opt.train_roi_h:\n",
    "    modules_not_build.append(\"roi_h_heads\")\n",
    "if not opt.est_bbox and not opt.est_kps:\n",
    "    modules_not_build.append(\"roi_bbox_heads\")\n",
    "sys.path.insert(0, \"models/pointnet\")\n",
    "print(modules_not_build)\n",
    "opt.debug = True\n",
    "model = RCNNOnly_combine(\n",
    "    opt, logger, printer, num_layers=opt.num_layers, modules_not_build=modules_not_build\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "model.turn_on_all_params()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    printer.print(name, param.shape, param.requires_grad)\n",
    "printer.print(\"ALL %d params\" % len(list(model.named_parameters())))\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), lr=cfg.SOLVER.BASE_LR, betas=(opt.beta1, 0.999), eps=1e-5\n",
    ")\n",
    "scheduler = ReduceLROnPlateau(optimizer, \"min\", factor=0.5, patience=100, cooldown=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CHECKPOINT\n",
    "opt.checkpoints_path_task = os.path.join(opt.checkpoints_folder, opt.task_name)\n",
    "save_to_disk = get_rank() == 0\n",
    "checkpointer = DetectronCheckpointer(\n",
    "    opt, model, optimizer, scheduler, opt.checkpoints_folder, opt.checkpoints_path_task, save_to_disk, logger=logger, if_print=False\n",
    ")\n",
    "tid_start = 0\n",
    "epoch_start = 0\n",
    "if opt.resume != 'NoCkpt':\n",
    "    checkpoint_restored, _, _ = checkpointer.load(task_name=opt.resume)\n",
    "    if 'iteration' in checkpoint_restored:\n",
    "        tid_start = checkpoint_restored['iteration']\n",
    "    if 'epoch' in checkpoint_restored:\n",
    "        epoch_start = checkpoint_restored['epoch']\n",
    "    print(checkpoint_restored.keys())\n",
    "    logger.info(colored('Restoring from epoch %d - iter %d'%(epoch_start, tid_start), 'white', 'on_blue'))\n",
    "model.print_net()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DATASET\n",
    "train_trnfs_maskrcnn = build_transforms_maskrcnn(cfg, True)\n",
    "eval_trnfs_maskrcnn = build_transforms_maskrcnn(cfg, False)\n",
    "train_trnfs_yannick = build_transforms_yannick(cfg, True)\n",
    "eval_trnfs_yannick = build_transforms_yannick(cfg, False)\n",
    "\n",
    "ds_train_coco_vis = COCO2017ECCV(\n",
    "    transforms_yannick=train_trnfs_yannick,\n",
    "    transforms_maskrcnn=train_trnfs_maskrcnn,\n",
    "    split=\"train\",\n",
    "    shuffle=False,\n",
    "    logger=logger,\n",
    "    opt=opt,\n",
    ")  \n",
    "ds_eval_coco_vis = COCO2017ECCV(\n",
    "    transforms_yannick=eval_trnfs_yannick,\n",
    "    transforms_maskrcnn=eval_trnfs_maskrcnn,\n",
    "    split=\"val\",\n",
    "    shuffle=False,\n",
    "    logger=logger,\n",
    "    opt=opt,\n",
    ")  \n",
    "\n",
    "training_loader_coco_vis = make_data_loader(\n",
    "    cfg,\n",
    "    ds_train_coco_vis,\n",
    "    is_train=True,\n",
    "    is_distributed=False,\n",
    "    start_iter=0,\n",
    "    logger=logger,\n",
    "    collate_fn=my_collate,\n",
    "    batch_size_override=2,  # BN does not make sense when model.train() and batchsize==1!\n",
    ")\n",
    "eval_loader_coco_vis = make_data_loader(\n",
    "    cfg,\n",
    "    ds_eval_coco_vis,\n",
    "    is_train=False,\n",
    "    is_distributed=False,\n",
    "    logger=logger,\n",
    "    collate_fn=my_collate,\n",
    "    batch_size_override=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = \"train_results\"\n",
    "Path(results_path).mkdir(exist_ok=True)\n",
    "task_name = opt.resume\n",
    "task_name_appendix = \"-predH\"\n",
    "task_name += task_name_appendix\n",
    "\n",
    "write_folder = os.path.join(results_path, task_name)\n",
    "for subfolder in [\"\", \"png\", \"npy\", \"pickle\", \"results\"]:\n",
    "    Path(os.path.join(write_folder, subfolder)).mkdir(parents=True, exist_ok=True)\n",
    "results_path_png = os.path.join(write_folder, \"png\")\n",
    "results_path_results = os.path.join(write_folder, \"results\")\n",
    "\n",
    "is_training = True\n",
    "if_vis = False\n",
    "if_debug = False\n",
    "prepostfix = \"trainSet-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_batch_combine_RCNNOnly_v5_pose_multiCat import train_batch_combine\n",
    "opt.zero_pitch = False\n",
    "if_vis = False\n",
    "if_blender = False\n",
    "select_show = 0\n",
    "\n",
    "## START TRAINING\n",
    "best_loss = float(\"inf\")\n",
    "bins = get_bins_combine(device)\n",
    "tid = 0\n",
    "\n",
    "epoch = 0\n",
    "epochs_evalued = []\n",
    "epochs_saved = []\n",
    "eval_loss = 0\n",
    "cont = 0\n",
    "\n",
    "model.train()\n",
    "\n",
    "cpu_device = torch.device(\"cpu\")\n",
    "loss_func = torch.nn.L1Loss()\n",
    "if opt.distributed:\n",
    "    rank = dist.get_rank()\n",
    "else:\n",
    "    rank = 0\n",
    "\n",
    "eval_loss_vt_list = []\n",
    "eval_loss_person_list = []\n",
    "vt_loss_allBoxes_dict = {}\n",
    "vt_loss_allBoxes_dict_list = []\n",
    "\n",
    "im_filename_list = []\n",
    "\n",
    "test_list = []\n",
    "\n",
    "vt_loss_all = []\n",
    "if_print = False\n",
    "if_debug = False\n",
    "cfg = opt.cfg\n",
    "\n",
    "num_plots = 0\n",
    "pitch_abs_list = []\n",
    "epochs = range(0, opt.nepoch)\n",
    "for epoch in epochs:\n",
    "    for i, (\n",
    "        _,\n",
    "        inputCOCO_Image_maskrcnnTransform_list,\n",
    "        W_batch_array,\n",
    "        H_batch_array,\n",
    "        yc_batch,\n",
    "        bboxes_batch_array,\n",
    "        bboxes_length_batch_array,\n",
    "        v0_batch,\n",
    "        f_pixels_yannick_batch,\n",
    "        im_filename,\n",
    "        im_file,\n",
    "        target_maskrcnnTransform_list,\n",
    "        labels_list,\n",
    "    ) in tqdm(enumerate(training_loader_coco_vis)):\n",
    "        tid = i\n",
    "        is_better = False\n",
    "        input_dict = {\n",
    "            \"inputCOCO_Image_maskrcnnTransform_list\": inputCOCO_Image_maskrcnnTransform_list,\n",
    "            \"W_batch_array\": W_batch_array,\n",
    "            \"H_batch_array\": H_batch_array,\n",
    "            \"yc_batch\": yc_batch,\n",
    "            \"bboxes_batch_array\": bboxes_batch_array,\n",
    "            \"bboxes_length_batch_array\": bboxes_length_batch_array,\n",
    "            \"v0_batch\": v0_batch,\n",
    "            \"f_pixels_yannick_batch\": f_pixels_yannick_batch,\n",
    "            \"im_filename\": im_filename,\n",
    "            \"im_file\": im_file,\n",
    "            \"bins\": bins,\n",
    "            \"target_maskrcnnTransform_list\": target_maskrcnnTransform_list,\n",
    "            \"labels_list\": labels_list,\n",
    "        }\n",
    "        bins = input_dict[\"bins\"]\n",
    "        return_dict, loss_dict = train_batch_combine(\n",
    "                    input_dict,\n",
    "                    model,\n",
    "                    device,\n",
    "                    opt,\n",
    "                    is_training=True,\n",
    "                    tid=i,\n",
    "                    loss_func=loss_func,\n",
    "                    rank=rank,\n",
    "                    if_SUN360=False,\n",
    "                    if_vis=False,\n",
    "        )\n",
    "        # dict_keys(['loss_kp', 'loss_bbox_cls', 'loss_bbox_reg', 'loss_vt', 'loss_person'])\n",
    "        # dict_keys(\n",
    "            # ['yc_est_batch', 'vt_loss_allBoxes_dict', 'loss_vt_list', 'loss_vt_layers_dict', \n",
    "            # 'all_person_hs', 'all_person_hs_layers', 'loss_all_person_h_list'])\n",
    "        # Combine the losses into a single scalar\n",
    "        a1, a2, a3, a4, a5 = 1.0, 0.05, 1.0, 10.0, 10.0\n",
    "        # NOTE: I need to add the loss from the Calibration dataset here for a3\n",
    "        total_loss = a1 * return_dict[\"loss_vt\"] + a2 * return_dict[\"loss_person\"] + a4 * return_dict[\"loss_bbox_cls\"] + a5 * return_dict[\"loss_kp\"]\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        if tid % opt.summary_every_iter == 0: \n",
    "            logger.info(\n",
    "                f\"Epoch {epoch} Iter {tid}: Loss VT = {loss_dict['loss_vt'].item():.4f} \" + \n",
    "                f\"Loss Person = {loss_dict['loss_person'].item():.4f}\"\n",
    "            )\n",
    "        if opt.save_every_iter != 0 and tid % opt.save_every_iter == 0 and tid > 0:\n",
    "            check_save(\n",
    "                rank=rank,\n",
    "                tid=tid,\n",
    "                epoch_save=epoch,\n",
    "                epoch_total=epoch,\n",
    "                opt=opt,\n",
    "                checkpointer=checkpointer,\n",
    "                epochs_saved=epochs_saved,\n",
    "                checkpoints_folder=opt.checkpoints_folder,\n",
    "                logger=logger,\n",
    "                is_better=is_better,\n",
    "            )\n",
    "        # After computing loss_dict and other stats for this tid/epoch\n",
    "        if tid != 0 and (\n",
    "            (opt.save_every_iter != 0 and tid % opt.save_every_iter == 0)\n",
    "            or tid % (len(training_loader_coco_vis)-1) == 0\n",
    "        ):\n",
    "            is_better = check_eval_COCO(\n",
    "                tid=tid,\n",
    "                epoch=epoch,\n",
    "                rank=rank,\n",
    "                opt=opt,\n",
    "                model=model,\n",
    "                eval_loader=eval_loader_coco_vis,\n",
    "                writer=writer,\n",
    "                device=device,\n",
    "                bins=bins,\n",
    "                logger=logger,\n",
    "                scheduler=scheduler,\n",
    "                epochs_evalued=epochs_evalued,\n",
    "            )\n",
    "\n",
    "            check_save(\n",
    "                rank=rank,\n",
    "                tid=tid,\n",
    "                epoch_save=epoch,\n",
    "                epoch_total=epoch,\n",
    "                opt=opt,\n",
    "                checkpointer=checkpointer,\n",
    "                epochs_saved=epochs_saved,\n",
    "                checkpoints_folder=opt.checkpoints_folder,\n",
    "                logger=logger,\n",
    "                is_better=is_better,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval-RELEASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = \"release_results\"\n",
    "\n",
    "Path(results_path).mkdir(exist_ok=True)\n",
    "task_name = opt.resume\n",
    "task_name_appendix = \"-predH\"\n",
    "task_name += task_name_appendix\n",
    "\n",
    "print(task_name)\n",
    "write_folder = os.path.join(results_path, task_name)\n",
    "for subfolder in [\"\", \"png\", \"npy\", \"pickle\", \"results\"]:\n",
    "    Path(os.path.join(write_folder, subfolder)).mkdir(parents=True, exist_ok=True)\n",
    "results_path_png = os.path.join(write_folder, \"png\")\n",
    "results_path_results = os.path.join(write_folder, \"results\")\n",
    "\n",
    "is_training = False\n",
    "if_vis = True\n",
    "if_blender = True\n",
    "if_debug = False\n",
    "prepostfix = \"testSet-|-evalMode\"\n",
    "\n",
    "test_loader = eval_loader_coco_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "opt.zero_pitch = False\n",
    "select_show = 0\n",
    "logger = setup_logger(\n",
    "    \"logger:train\", summary_path, get_rank(), filename=\"logger_maskrcn-style.txt\"\n",
    ")\n",
    "## START TRAINING\n",
    "best_loss = float(\"inf\")\n",
    "bins = get_bins_combine(device)\n",
    "tid = 0\n",
    "\n",
    "epoch = 0\n",
    "eval_loss = 0\n",
    "\n",
    "model.eval()\n",
    "\n",
    "cpu_device = torch.device(\"cpu\")\n",
    "loss_func = torch.nn.L1Loss()\n",
    "if opt.distributed:\n",
    "    rank = dist.get_rank()\n",
    "else:\n",
    "    rank = 0\n",
    "\n",
    "eval_loss_vt_list = []\n",
    "eval_loss_person_list = []\n",
    "vt_loss_allBoxes_dict = {}\n",
    "vt_loss_allBoxes_dict_list = []\n",
    "\n",
    "im_filename_list = []\n",
    "\n",
    "test_list = []\n",
    "\n",
    "vt_loss_all = []\n",
    "\n",
    "num_plots = 0\n",
    "pitch_abs_list = []\n",
    "with torch.no_grad():\n",
    "    for i, (\n",
    "        _,\n",
    "        inputCOCO_Image_maskrcnnTransform_list,\n",
    "        W_batch_array,\n",
    "        H_batch_array,\n",
    "        yc_batch,\n",
    "        bboxes_batch_array,\n",
    "        bboxes_length_batch_array,\n",
    "        v0_batch,\n",
    "        f_pixels_yannick_batch,\n",
    "        im_filename,\n",
    "        im_file,\n",
    "        target_maskrcnnTransform_list,\n",
    "        labels_list,\n",
    "    ) in tqdm(enumerate(test_loader)):\n",
    "\n",
    "        if select_show != -1 and i < select_show:\n",
    "            continue\n",
    "        tid = i\n",
    "\n",
    "        input_dict = {\n",
    "            \"inputCOCO_Image_maskrcnnTransform_list\": inputCOCO_Image_maskrcnnTransform_list,\n",
    "            \"W_batch_array\": W_batch_array,\n",
    "            \"H_batch_array\": H_batch_array,\n",
    "            \"yc_batch\": yc_batch,\n",
    "            \"bboxes_batch_array\": bboxes_batch_array,\n",
    "            \"bboxes_length_batch_array\": bboxes_length_batch_array,\n",
    "            \"v0_batch\": v0_batch,\n",
    "            \"f_pixels_yannick_batch\": f_pixels_yannick_batch,\n",
    "            \"im_filename\": im_filename,\n",
    "            \"im_file\": im_file,\n",
    "            \"bins\": bins,\n",
    "            \"target_maskrcnnTransform_list\": target_maskrcnnTransform_list,\n",
    "            \"labels_list\": labels_list,\n",
    "        }\n",
    "        if_print = is_training\n",
    "        cfg = opt.cfg\n",
    "        bins = input_dict[\"bins\"]\n",
    "\n",
    "        # ========= Rui's inputs\n",
    "        inputCOCO_Image_maskrcnnTransform_list = input_dict[\n",
    "            \"inputCOCO_Image_maskrcnnTransform_list\"\n",
    "        ]\n",
    "        (\n",
    "            bboxes_batch,\n",
    "            v0_batch_offline,\n",
    "            f_pixels_yannick_batch_offline,\n",
    "            W_batch,\n",
    "            H_batch,\n",
    "            yc_batch_offline,\n",
    "        ) = (\n",
    "            torch.from_numpy(input_dict[\"bboxes_batch_array\"]).float().to(device),\n",
    "            input_dict[\"v0_batch\"].to(device),\n",
    "            input_dict[\"f_pixels_yannick_batch\"].to(device),\n",
    "            torch.from_numpy(input_dict[\"W_batch_array\"]).to(device),\n",
    "            torch.from_numpy(input_dict[\"H_batch_array\"]).to(device),\n",
    "            input_dict[\"yc_batch\"].to(device),\n",
    "        )\n",
    "\n",
    "        if not opt.not_rcnn:\n",
    "            list_of_bbox_list_cpu = model_utils.bboxArray_to_bboxList(\n",
    "                bboxes_batch,\n",
    "                input_dict[\"bboxes_length_batch_array\"],\n",
    "                input_dict[\"W_batch_array\"],\n",
    "                input_dict[\"H_batch_array\"],\n",
    "            )\n",
    "        else:\n",
    "            list_of_bbox_list_cpu = []\n",
    "\n",
    "        if if_vis:\n",
    "            input_dict_show = {'H': input_dict['H_batch_array'], 'W': input_dict['W_batch_array']}\n",
    "            if 'testSet' not in prepostfix:\n",
    "                input_dict_show.update({'v0_cocoPredict': input_dict['v0_batch'].numpy()})\n",
    "            input_dict_show['bbox_gt'] = []\n",
    "            input_dict_show['bbox_est'] = []\n",
    "            input_dict_show['bbox_fit'] = []\n",
    "            input_dict_show['bbox_h'] = []\n",
    "            input_dict_show['bbox_geo'] = []\n",
    "            input_dict_show['bbox_loss'] = []\n",
    "\n",
    "\n",
    "        list_of_oneLargeBbox_list_cpu = model_utils.oneLargeBboxList(\n",
    "            input_dict[\"W_batch_array\"], input_dict[\"H_batch_array\"]\n",
    "        )  # in original image size\n",
    "        list_of_oneLargeBbox_list = [\n",
    "            bbox_list_array.to(device)\n",
    "            for bbox_list_array in list_of_oneLargeBbox_list_cpu\n",
    "        ]\n",
    "\n",
    "        input_dict_misc = {\n",
    "            \"bins\": bins,\n",
    "            \"is_training\": is_training,\n",
    "            \"H_batch\": H_batch,\n",
    "            \"W_batch\": W_batch,\n",
    "            \"bboxes_batch\": bboxes_batch,\n",
    "            \"loss_func\": loss_func,\n",
    "            \"cpu_device\": cpu_device,\n",
    "            \"device\": device,\n",
    "            \"tid\": tid,\n",
    "            \"rank\": rank,\n",
    "            \"data\": \"coco\",\n",
    "            \"if_vis\": if_vis,\n",
    "        }\n",
    "        output_RCNN = model(\n",
    "            input_dict_misc=input_dict_misc,\n",
    "            input_dict=input_dict,\n",
    "            image_batch_list=inputCOCO_Image_maskrcnnTransform_list,\n",
    "            list_of_bbox_list_cpu=list_of_bbox_list_cpu,\n",
    "            list_of_oneLargeBbox_list=list_of_oneLargeBbox_list,\n",
    "        )\n",
    "        pitch_abs_degree = abs(output_RCNN[\"pitch_batch_est\"][0].item()) / np.pi * 180.0\n",
    "        pitch_abs_list.append(pitch_abs_degree)\n",
    "        if \"zero_pitch\" in opt and opt.zero_pitch:\n",
    "            output_RCNN[\"pitch_batch_est\"] *= 0.0\n",
    "            output_RCNN[\"output_pitch\"] *= 0.0\n",
    "            output_RCNN[\"v0_batch_from_pitch_vfov\"] = (\n",
    "                output_RCNN[\"v0_batch_from_pitch_vfov\"] * 0.0 + H_batch[0] / 2.0\n",
    "            )\n",
    "            output_RCNN[\"v0_batch_est\"] = output_RCNN[\"v0_batch_from_pitch_vfov\"]\n",
    "\n",
    "        output_horizon = output_RCNN[\"output_horizon\"]\n",
    "        output_pitch = output_RCNN[\"output_pitch\"]\n",
    "        output_vfov = output_RCNN[\"output_vfov\"]\n",
    "        output_yc_batch = output_RCNN[\"output_yc_batch\"]\n",
    "        f_pixels_yannick_batch_ori = f_pixels_yannick_batch_offline.clone()\n",
    "        v0_batch_ori = v0_batch_offline.clone()\n",
    "\n",
    "        v0_batch_est = output_RCNN[\"v0_batch_est\"]\n",
    "        v0_batch_from_pitch_vfov = output_RCNN[\"v0_batch_from_pitch_vfov\"]\n",
    "\n",
    "        vfov_estim = output_RCNN[\"vfov_estim\"]\n",
    "        f_pixels_yannick_batch_est = output_RCNN[\"f_pixels_batch_est\"]\n",
    "\n",
    "        pitch_batch_est = output_RCNN[\"pitch_batch_est\"]\n",
    "        pitch_estim_yannick = output_RCNN[\"pitch_estim_yannick\"]\n",
    "\n",
    "        yc_est_batch = output_RCNN[\"yc_est_batch\"]\n",
    "\n",
    "        person_h_list = output_RCNN[\"person_h_list\"]\n",
    "        all_person_hs = output_RCNN[\"all_person_hs\"]\n",
    "\n",
    "        reduce_method = output_RCNN[\"reduce_method\"]\n",
    "\n",
    "        if tid % opt.summary_every_iter == 0 and if_print:\n",
    "            f_mm_array_est = f_pixels_to_mm(f_pixels_yannick_batch_est, input_dict)\n",
    "\n",
    "        ## Losses for bbox fitting\n",
    "        vt_loss_sample_batch_list = []\n",
    "        vt_loss_allBoxes_dict_cpu = {}\n",
    "        vt_error_fit_allBoxes_dict_cpu = {}\n",
    "        camH_fit_batch = []\n",
    "\n",
    "        for idx, bboxes_length in enumerate(input_dict[\"bboxes_length_batch_array\"]):\n",
    "            bboxes = bboxes_batch[idx][:bboxes_length]  # [N, 4]\n",
    "            W = W_batch[idx]\n",
    "            H = H_batch[idx]\n",
    "            vc = H / 2.0\n",
    "            v0_est = v0_batch_est[idx]\n",
    "            v0_ori = v0_batch_ori[idx]  # [top H, bottom 0]\n",
    "            pitch_est = pitch_batch_est[idx]\n",
    "            f_pixels_yannick_est = f_pixels_yannick_batch_est[idx]\n",
    "            inv_f2 = 1.0 / (f_pixels_yannick_est * f_pixels_yannick_est)\n",
    "            yc_est = yc_est_batch[idx]\n",
    "\n",
    "            H_np = H_batch[idx].cpu().numpy()\n",
    "            W_np = W_batch[idx].cpu().numpy()\n",
    "\n",
    "            if opt.not_rcnn:\n",
    "                h_human_s = (\n",
    "                    torch.from_numpy(\n",
    "                        np.asarray([1.75] * bboxes.shape[0], dtype=np.float32)\n",
    "                    )\n",
    "                    .float()\n",
    "                    .to(device)\n",
    "                )\n",
    "            else:\n",
    "                h_human_s = (\n",
    "                    person_h_list[idx] * output_RCNN[\"straighten_ratios_list\"][idx]\n",
    "                )\n",
    "\n",
    "            vt_loss_sample_list = []\n",
    "            vt_error_sample_fit_list = []\n",
    "\n",
    "            camH_fit_list = []\n",
    "            for bbox_idx, bbox in enumerate(bboxes):\n",
    "                y_person_fit = 1.75\n",
    "                camH_fit_bbox = geo_utils.fit_camH(\n",
    "                    bbox.cpu(),\n",
    "                    H.cpu(),\n",
    "                    v0_est.cpu(),\n",
    "                    vc.cpu(),\n",
    "                    f_pixels_yannick_est.cpu(),\n",
    "                    y_person_fit,\n",
    "                )\n",
    "                camH_fit_list.append(camH_fit_bbox.detach().numpy())\n",
    "            camH_fit = np.median(np.array(camH_fit_list))\n",
    "            camH_fit_batch.append(camH_fit)\n",
    "\n",
    "            bbox_gt_sample = []\n",
    "            bbox_est_sample = []\n",
    "            bbox_fit_sample = []\n",
    "            bbox_h_sample = []\n",
    "            bbox_geo_sample = []\n",
    "            bbox_loss_sample = []\n",
    "            for bbox_idx, (bbox, y_person) in enumerate(zip(bboxes, h_human_s)):\n",
    "                vb = H - (bbox[1] + bbox[3])  # [top H bottom 0]\n",
    "                vt_gt = H - bbox[1]  # [top H bottom 0]\n",
    "\n",
    "                ## Fit y_person\n",
    "                geo_model_input_dict = {\n",
    "                    \"yc_est\": yc_est,\n",
    "                    \"vb\": vb,\n",
    "                    \"y_person\": y_person,\n",
    "                    \"v0\": v0_est,\n",
    "                    \"vc\": vc,\n",
    "                    \"f_pixels_yannick\": f_pixels_yannick_est,\n",
    "                    \"pitch_est\": pitch_est,\n",
    "                }\n",
    "                negative_z = False\n",
    "                if opt.accu_model:\n",
    "                    vt_camEst, z, negative_z = model_utils.accu_model_helanyi(\n",
    "                        geo_model_input_dict, if_debug=False\n",
    "                    )  # [top H bottom 0]\n",
    "                    geo_model_input_dict_rui = {\n",
    "                        \"yc_est\": yc_est,\n",
    "                        \"vb\": vb,\n",
    "                        \"y_person\": y_person * torch.cos(pitch_est),\n",
    "                        \"v0\": v0_est,\n",
    "                        \"vc\": vc,\n",
    "                        \"f_pixels_yannick\": f_pixels_yannick_est,\n",
    "                        \"pitch_est\": pitch_est,\n",
    "                    }\n",
    "                    vt_camEst_rui, z_rui, negative_z_rui = model_utils.accu_model(\n",
    "                        geo_model_input_dict_rui, if_debug=False\n",
    "                    )  # [top H bottom 0]\n",
    "                else:\n",
    "                    vt_camEst = model_utils.approx_model(geo_model_input_dict)\n",
    "\n",
    "                vt_loss = loss_func(vt_gt, vt_camEst.reshape([])) / bbox[3]\n",
    "                vt_loss = torch.clamp(vt_loss, 0.0, opt.cfg.MODEL.LOSS.VT_LOSS_CLAMP)\n",
    "                vt_loss = torch.where(\n",
    "                    torch.isnan(vt_loss), torch.zeros_like(vt_loss), vt_loss\n",
    "                )\n",
    "                vt_loss_sample_list.append(vt_loss)\n",
    "                vt_loss_allBoxes_dict_cpu.update(\n",
    "                    {\n",
    "                        \"bbox_vt_loss_tid%04d_rank%d_%02d-%02d\"\n",
    "                        % (tid, rank, idx, bbox_idx): vt_loss.to(cpu_device)\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                # Fitting 现场: getting vt\n",
    "                y_person_fit = 1.75\n",
    "                vt_camFit = geo_utils.fit_vt(\n",
    "                    camH_fit,\n",
    "                    vb,\n",
    "                    v0_est,\n",
    "                    vc,\n",
    "                    y_person_fit,\n",
    "                    1.0 / (f_pixels_yannick_est * f_pixels_yannick_est),\n",
    "                )\n",
    "                vt_error_fit = loss_func(vt_gt, vt_camFit) / bbox[3]\n",
    "\n",
    "                vt_error_fit_allBoxes_dict_cpu.update(\n",
    "                    {\n",
    "                        \"bbox_vt_error_fit_tid%04d_rank%d_%02d-%02d\"\n",
    "                        % (tid, rank, idx, bbox_idx): vt_error_fit.to(cpu_device)\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                vt_error_sample_fit_list.append(\n",
    "                    vt_error_fit.detach().cpu().numpy().item()\n",
    "                )\n",
    "\n",
    "                if if_vis:\n",
    "                    bbox_np = bbox.cpu().numpy()\n",
    "                    vt_camEst_np = vt_camEst.detach().cpu().numpy()\n",
    "                    vt_camFit_np = vt_camFit.detach().cpu().numpy()\n",
    "                    bbox_gt_sample.append(\n",
    "                        [bbox_np[0], bbox_np[1], bbox_np[2], bbox_np[3]]\n",
    "                    )  # [x, y (top), w, h]\n",
    "                    bbox_est_sample.append(\n",
    "                        [\n",
    "                            bbox_np[0],\n",
    "                            H_np - vt_camEst_np,\n",
    "                            bbox_np[2],\n",
    "                            bbox_np[1] + bbox_np[3] - (H_np - vt_camEst_np),\n",
    "                        ]\n",
    "                    )\n",
    "                    bbox_h_sample.append(y_person.cpu().detach().numpy())\n",
    "                    bbox_geo_sample.append(geo_model_input_dict)\n",
    "                    bbox_loss_sample.append(vt_loss.item())\n",
    "\n",
    "            if if_vis:\n",
    "                input_dict_show[\"bbox_gt\"].append(bbox_gt_sample)\n",
    "                input_dict_show[\"bbox_est\"].append(bbox_est_sample)\n",
    "                input_dict_show[\"bbox_fit\"].append(bbox_fit_sample)\n",
    "                input_dict_show[\"bbox_h\"].append(bbox_h_sample)\n",
    "                input_dict_show[\"bbox_geo\"].append(bbox_geo_sample)\n",
    "                input_dict_show[\"bbox_loss\"].append(bbox_loss_sample)\n",
    "\n",
    "            vt_loss_sample = torch.mean(torch.stack(vt_loss_sample_list))\n",
    "            vt_loss_sample_batch_list.append(vt_loss_sample)\n",
    "\n",
    "        vt_loss_batch = torch.stack(vt_loss_sample_batch_list)\n",
    "        loss_vt = torch.mean(vt_loss_batch)\n",
    "\n",
    "        return_dict = {\n",
    "            \"vt_loss_batch\": vt_loss_batch,\n",
    "            \"vt_loss_allBoxes_dict\": vt_loss_allBoxes_dict_cpu,\n",
    "            \"vt_error_fit_allBoxes_dict\": vt_error_fit_allBoxes_dict_cpu,\n",
    "            \"yc_est_batch\": yc_est_batch,\n",
    "            \"yc_batch_offline\": yc_batch_offline,\n",
    "            \"yc_fit_batch\": np.array(camH_fit_batch),\n",
    "        }\n",
    "        vt_loss_allBoxes_dict_list.append(vt_loss_allBoxes_dict_cpu)\n",
    "        if tid % opt.summary_every_iter == 0 and if_print:\n",
    "            return_dict.update({\"f_mm_batch\": f_mm_array_est.reshape(-1, 1)})\n",
    "\n",
    "        if \"loss_vt_list\" in output_RCNN:\n",
    "            loss_vt_list = output_RCNN[\"loss_vt_list\"]\n",
    "            assert len(loss_vt_list) != 0\n",
    "        else:\n",
    "            loss_vt_list = []\n",
    "        loss_vt_list.append(loss_vt)\n",
    "        return_dict.update({\"loss_vt_list\": loss_vt_list})\n",
    "        loss_dict = {\n",
    "            \"loss_vt\": sum(loss_vt_list) / len(loss_vt_list)\n",
    "        }  # mean of layers; for optimization and scheduler\n",
    "\n",
    "        if opt.pointnet_camH_refine:\n",
    "            loss_vt_layers_dict = {}\n",
    "            for loss_idx, loss in enumerate(loss_vt_list):\n",
    "                loss_vt_layers_dict[\n",
    "                    \"loss_vt_layer_%d\" % (loss_idx - len(loss_vt_list))\n",
    "                ] = loss\n",
    "\n",
    "            return_dict.update({\"loss_vt_layers_dict\": loss_vt_layers_dict})\n",
    "\n",
    "        if not opt.not_rcnn:\n",
    "            return_dict.update({\"all_person_hs\": all_person_hs})\n",
    "            loss_all_person_h_list = output_RCNN[\"loss_all_person_h_list\"]\n",
    "            return_dict.update({\"loss_all_person_h_list\": loss_all_person_h_list})\n",
    "            loss_dict.update(\n",
    "                {\n",
    "                    \"loss_person\": sum(loss_all_person_h_list)\n",
    "                    / len(loss_all_person_h_list)\n",
    "                }\n",
    "            )  # mean of layers; for optimization and scheduler\n",
    "\n",
    "        # ========== Some vis\n",
    "        input_dict_show[\"W_batch_array\"] = input_dict[\"W_batch_array\"]\n",
    "        input_dict_show[\"H_batch_array\"] = input_dict[\"H_batch_array\"]\n",
    "        if if_vis:\n",
    "            if opt.est_kps:\n",
    "                input_dict_show[\"predictions\"] = output_RCNN[\"predictions\"]\n",
    "                input_dict_show[\"target_maskrcnnTransform_list\"] = input_dict[\n",
    "                    \"target_maskrcnnTransform_list\"\n",
    "                ]\n",
    "            #             input_dict_show['v0_batch_predict'] = v0_batch_predict.detach().cpu().numpy()  # (H = top of the image, 0 = bottom of the image)\n",
    "            input_dict_show[\"v0_batch_from_pitch_vfov\"] = (\n",
    "                v0_batch_from_pitch_vfov.detach().cpu().numpy()\n",
    "            )\n",
    "            input_dict_show[\"v0_batch_est\"] = v0_batch_est.detach().cpu().numpy()\n",
    "            if \"v0_batch_est_0\" in output_RCNN:\n",
    "                input_dict_show[\"v0_batch_est_0\"] = (\n",
    "                    output_RCNN[\"v0_batch_est_0\"].detach().cpu().numpy()\n",
    "                )\n",
    "            f_pixels_yannick_single_est = (\n",
    "                f_pixels_yannick_batch_est.detach().cpu().numpy()\n",
    "            )\n",
    "            f_pixels_yannick_single_est_mm = [\n",
    "                fpix_to_fmm(f_pixels_yannick_single_est_0, H_np, W_np)\n",
    "                for f_pixels_yannick_single_est_0 in f_pixels_yannick_single_est\n",
    "            ]\n",
    "            f_pixels_yannick_single_ori = (\n",
    "                f_pixels_yannick_batch_ori.detach().cpu().numpy()\n",
    "            )\n",
    "            f_pixels_yannick_single_ori_mm = [\n",
    "                fpix_to_fmm(f_pixels_yannick_single_ori_0, H_np, W_np)\n",
    "                for f_pixels_yannick_single_ori_0 in f_pixels_yannick_single_ori\n",
    "            ]\n",
    "            input_dict_show.update(\n",
    "                {\n",
    "                    \"yc_fit\": yc_batch_offline.detach().cpu().numpy(),\n",
    "                    \"yc_est\": yc_est_batch.detach().cpu().numpy(),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if len(output_RCNN[\"yc_est_batch_np_list\"]) > 1:  # more than one layers\n",
    "                input_dict_show.update(\n",
    "                    {\"yc_est_list\": output_RCNN[\"yc_est_batch_np_list\"]}\n",
    "                )\n",
    "            if len(output_RCNN[\"person_hs_est_np_list\"]) > 1:\n",
    "                input_dict_show.update(\n",
    "                    {\"person_hs_est_list\": output_RCNN[\"person_hs_est_np_list\"]}\n",
    "                )\n",
    "            if (\n",
    "                \"vt_camEst_N_delta_np_list\" in output_RCNN\n",
    "                and len(output_RCNN[\"vt_camEst_N_delta_np_list\"]) > 1\n",
    "            ):\n",
    "                input_dict_show.update(\n",
    "                    {\n",
    "                        \"vt_camEst_N_delta_est_list\": output_RCNN[\n",
    "                            \"vt_camEst_N_delta_np_list\"\n",
    "                        ]\n",
    "                    }\n",
    "                )\n",
    "            if (\n",
    "                len(output_RCNN[\"f_pixels_est_batch_np_list\"]) > 1\n",
    "            ):  # more than one layers\n",
    "                input_dict_show.update(\n",
    "                    {\n",
    "                        \"f_pixels_est_mm_list\": [\n",
    "                            fpix_to_fmm(f_pixels_est_0, H_np, W_np)\n",
    "                            for f_pixels_est_0 in output_RCNN[\n",
    "                                \"f_pixels_est_batch_np_list\"\n",
    "                            ]\n",
    "                        ]\n",
    "                    }\n",
    "                )\n",
    "            if len(output_RCNN[\"v0_01_est_batch_np_list\"]) > 1:  # more than one layers\n",
    "                input_dict_show.update(\n",
    "                    {\"v0_est_list\": output_RCNN[\"v0_01_est_batch_np_list\"]}\n",
    "                )\n",
    "\n",
    "            input_dict_show.update(\n",
    "                {\n",
    "                    \"f_est_px\": f_pixels_yannick_single_est,\n",
    "                    \"f_est_mm\": f_pixels_yannick_single_est_mm,\n",
    "                }\n",
    "            )\n",
    "            input_dict_show.update(\n",
    "                {\n",
    "                    \"f_cocoPredict\": f_pixels_yannick_single_ori,\n",
    "                    \"f_cocoPredict_mm\": f_pixels_yannick_single_ori_mm,\n",
    "                }\n",
    "            )\n",
    "            input_dict_show.update(\n",
    "                {\n",
    "                    \"pitch_est_angle\": pitch_batch_est.detach().cpu().numpy()\n",
    "                    / np.pi\n",
    "                    * 180.0\n",
    "                }\n",
    "            )\n",
    "\n",
    "            input_dict_show[\"im_path\"] = list(input_dict[\"im_file\"])\n",
    "            input_dict_show[\"im_filename\"] = list(input_dict[\"im_filename\"])\n",
    "            num_samples = len(input_dict[\"im_file\"])\n",
    "            input_dict_show[\"output_horizon_COCO\"] = (\n",
    "                output_RCNN[\"output_horizon\"].detach().cpu().numpy()\n",
    "            )\n",
    "            input_dict_show[\"horizon_bins\"] = [\n",
    "                bins[\"horizon_bins_centers_torch\"].cpu().numpy()\n",
    "            ] * num_samples\n",
    "\n",
    "            if not opt.direct_camH:\n",
    "                input_dict_show[\"output_camH_COCO\"] = (\n",
    "                    output_RCNN[\"output_yc_batch\"].detach().cpu().numpy()\n",
    "                )\n",
    "                input_dict_show[\"camH_bins\"] = [\n",
    "                    bins[\"yc_bins_centers_torch\"].cpu().numpy()\n",
    "                ] * num_samples\n",
    "\n",
    "            input_dict_show[\"tid\"] = [tid] * num_samples\n",
    "            input_dict_show[\"task_name\"] = [opt.task_name] * num_samples\n",
    "            input_dict_show[\"num_samples\"] = num_samples\n",
    "            input_dict_show[\"reduce_method\"] = [reduce_method] * num_samples\n",
    "            input_dict_show.update(\n",
    "                {\n",
    "                    \"vfov_est\": vfov_estim.detach().cpu().numpy(),\n",
    "                    \"pitch_est_yannick\": pitch_estim_yannick.detach().cpu().numpy(),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if input_dict_show[\"num_samples\"] > 0:\n",
    "                input_dict_show_list = batch_dict_to_list_of_dicts(input_dict_show)\n",
    "                input_dict_show = input_dict_show_list[0]\n",
    "\n",
    "        prefix, postfix = prepostfix.split(\"|\")\n",
    "\n",
    "        save_path_blender = results_path_png\n",
    "        save_path_ours = results_path_png\n",
    "        prefix = \"coco_\" + prefix\n",
    "        postfix += \"_reproj\"\n",
    "\n",
    "        if if_vis:\n",
    "            show_cam_bbox(\n",
    "                io.imread(input_dict_show[\"im_path\"]),\n",
    "                input_dict_show,\n",
    "                figzoom=1.5,\n",
    "                if_show=True,\n",
    "                if_save=True if save_path_ours else False,\n",
    "                if_pause=False,\n",
    "                save_path=save_path_ours,\n",
    "                save_name=prefix + \"%06d\" % (tid) + postfix + \"_reproj\",\n",
    "            )\n",
    "            if opt.est_kps:\n",
    "                show_box_kps(\n",
    "                    opt,\n",
    "                    model,\n",
    "                    io.imread(input_dict_show[\"im_path\"]),\n",
    "                    input_dict_show,\n",
    "                    if_show=True,\n",
    "                    if_pause=False,\n",
    "                    save_path=\"\",\n",
    "                    save_name=prefix + \"tid%d\" % (tid) + postfix,\n",
    "                )\n",
    "\n",
    "            if if_blender:\n",
    "                tmp_code = base64.b32encode(im_filename[0].encode()).decode()\n",
    "                blender_render(\n",
    "                    input_dict_show,\n",
    "                    output_RCNN,\n",
    "                    im_file,\n",
    "                    tmp_code=tmp_code,\n",
    "                    render_type=\"chair\",\n",
    "                    pick=-1,\n",
    "                    grid=False,\n",
    "                    current_dir=os.path.join(os.getcwd()),\n",
    "                    save_name=tmp_code,\n",
    "                )\n",
    "        num_plots += 1\n",
    "        if num_plots >= 1:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

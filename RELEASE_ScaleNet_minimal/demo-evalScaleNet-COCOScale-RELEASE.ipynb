{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "\n",
    "import os,sys,inspect\n",
    "# currentdir = '/home/ruizhu/Documents/Projects/adobe_rui_camera-calibration-redux'\n",
    "current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "print(current_dir)\n",
    "sys.path.insert(0, current_dir)\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "torch_version = torch.__version__\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from utils.train_utils import *\n",
    "from models.model_RCNNOnly_combine_indeptPointnet_maskrcnnPose_discount import RCNNOnly_combine\n",
    "from dataset_coco_pickle_eccv import my_collate, COCO2017Scale\n",
    "from utils.data_utils import make_data_loader\n",
    "from maskrcnn_rui.data.transforms import build_transforms_maskrcnn, build_transforms_yannick\n",
    "from utils.logger import setup_logger, printer\n",
    "\n",
    "from maskrcnn_rui.config import cfg\n",
    "from maskrcnn_rui.utils.comm import get_rank\n",
    "import utils.vis_utils as vis_utils\n",
    "from utils.checkpointer import DetectronCheckpointer\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Rui's Scale Estimation Network Training\")\n",
    "# Training\n",
    "parser.add_argument('--task_name', type=str, default='tmp', help='resume training')\n",
    "parser.add_argument('--workers', type=int, help='number of data loading workers', default=8)\n",
    "parser.add_argument('--save_every_iter', type=int, default=0, help='set to 0 to save ONLY at the end of each epoch')\n",
    "parser.add_argument('--summary_every_iter', type=int, default=20, help='')\n",
    "parser.add_argument('--nepoch', type=int, default=15000, help='number of epochs to train for')\n",
    "parser.add_argument('--beta1', type=float, default=0.9, help='beta1 for adam. default=0.5')\n",
    "parser.add_argument('--not_val', action='store_true', help='Do not validate duruign training')\n",
    "parser.add_argument('--not_vis', action='store_true', help='')\n",
    "parser.add_argument('--not_vis_SUN360', action='store_true', help='')\n",
    "parser.add_argument('--save_every_epoch', type=int, default=10, help='save checkpoint every ? epoch')\n",
    "parser.add_argument('--vis_every_epoch', type=int, default=5, help='vis every ? epoch')\n",
    "# Model\n",
    "parser.add_argument('--accu_model', action='store_true', help='Use accurate model with theta instead of Derek\\'s approx.')\n",
    "parser.add_argument('--argmax_val', action='store_true', help='')\n",
    "parser.add_argument('--direct_camH', action='store_true', help='direct preidict one number for camera height ONLY, instead of predicting a distribution')\n",
    "parser.add_argument('--direct_v0', action='store_true', help='direct preidict one number for v0 ONLY, instead of predicting a distribution')\n",
    "parser.add_argument('--direct_fmm', action='store_true', help='direct preidict one number for fmm ONLY, instead of predicting a distribution')\n",
    "\n",
    "# Pre-training\n",
    "parser.add_argument('--resume', type=str, help='resume training; can be full path (e.g. tmp/checkpoint0.pth.tar) or taskname (e.g. tmp)', default='NoCkpt')\n",
    "parser.add_argument('--feature_only', action='store_true', help='restore only features (remove all classifiers) from checkpoint')\n",
    "parser.add_argument('--reset_scheduler', action='store_true', help='') # NOT working yet\n",
    "parser.add_argument('--reset_lr', action='store_true', help='') # NOT working yet\n",
    "\n",
    "# Device\n",
    "parser.add_argument('--cpu', action='store_true', help='Force training on CPU')\n",
    "parser.add_argument(\"--local_rank\", type=int, default=0)\n",
    "parser.add_argument(\"--master_port\", type=str, default='8914')\n",
    "\n",
    "# DEBUG\n",
    "parser.add_argument('--debug', action='store_true', help='Debug eval')\n",
    "parser.add_argument('--debug_memory', action='store_true', help='Debug eval')\n",
    "\n",
    "# Mask R-CNN\n",
    "## Modules\n",
    "parser.add_argument('--train_cameraCls', action='store_true', help='Disable camera calibration network')\n",
    "parser.add_argument('--train_roi_h', action='store_true', help='')\n",
    "parser.add_argument('--est_bbox', action='store_true', help='Enable estimating bboxes instead of using GT bboxes')\n",
    "parser.add_argument('--est_kps', action='store_true', help='Enable estimating keypoints')\n",
    "parser.add_argument('--if_discount', action='store_true', help='')\n",
    "parser.add_argument('--discount_from', type=str, default='GT') # ('GT', 'pred')\n",
    "\n",
    "## Losses\n",
    "parser.add_argument('--loss_last_layer', action='store_true', help='Using loss of last layer only')\n",
    "parser.add_argument('--loss_person_all_layers', action='store_true', help='Using loss of last layer only')\n",
    "parser.add_argument('--not_rcnn', action='store_true', help='Disable Mask R-CNN person height bbox head')\n",
    "parser.add_argument('--no_kps_loss', action='store_true', help='')\n",
    "\n",
    "## Archs\n",
    "parser.add_argument('--pointnet_camH', action='store_true', help='')\n",
    "parser.add_argument('--pointnet_camH_refine', action='store_true', help='')\n",
    "parser.add_argument('--pointnet_personH_refine', action='store_true', help='')\n",
    "parser.add_argument('--pointnet_roi_feat_input', action='store_true', help='') # NOT working yet\n",
    "parser.add_argument('--pointnet_roi_feat_input_person3', action='store_true', help='') # NOT working yet\n",
    "parser.add_argument('--pointnet_fmm_refine', action='store_true', help='') # NOT working yet\n",
    "parser.add_argument('--pointnet_v0_refine', action='store_true', help='') # NOT working yet\n",
    "parser.add_argument('--not_pointnet_detach_input', action='store_true', help='') # NOT working yet\n",
    "parser.add_argument(\"--num_layers\", type=int, default=3)\n",
    "parser.add_argument('--fit_derek', action='store_true', help='')\n",
    "## weights\n",
    "parser.add_argument('--weight_SUN360', type=float, default=10., help='weight for Yannick\\'s losses. default=1.')\n",
    "parser.add_argument('--weight_kps', type=float, default=1e-3, help='weight for Yannick\\'s losses. default=1.')\n",
    "\n",
    "## debug\n",
    "parser.add_argument('--zero_pitch', action='store_true', help='') # NOT working yet\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--config-file\",\n",
    "    default=\"\",\n",
    "    metavar=\"FILE\",\n",
    "    help=\"path to config file\",\n",
    "    type=str,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"opts\",\n",
    "    help=\"Modify config options using the command-line\",\n",
    "    default=None,\n",
    "    nargs=argparse.REMAINDER,\n",
    ")\n",
    "\n",
    "# [2.1-SN-L3] 20200222-162430_pod_backCompat_adam_wPerson05_720-540_REafterDeathV_afterFaster_bs16_fix3_nokpsLoss_personLoss3Layers_loss3layers\n",
    "opt = parser.parse_args('--task_name tmp_eval --num_layers 3 \\\n",
    "--train_cameraCls --train_roi_h --pointnet_camH --pointnet_camH_refine --pointnet_personH_refine --accu_model --no_kps_loss --loss_person_all_layers \\\n",
    "--config-file coco_config_small_synBN1108_kps.yaml  --weight_SUN360=10. \\\n",
    "MODEL.LOSS.VT_LOSS_CLAMP 2. SOLVER.IMS_PER_BATCH 16 TEST.IMS_PER_BATCH 16 SOLVER.PERSON_WEIGHT 0.05 SOLVER.BASE_LR 1e-5 MODEL.HUMAN.MEAN 1.70 MODEL.HUMAN.STD 0.15 MODEL.RCNN_WEIGHT_BACKBONE 0212-2047_mm2_SUN360RCNN_LossposeCamera_lr1e-5_NOsubsample-prepare_targets_for_gt_box_input-removeOutOfBox_wKps1-SUN10_RE MODEL.RCNN_WEIGHT_CLS_HEAD 0212-2047_mm2_SUN360RCNN_LossposeCamera_lr1e-5_NOsubsample-prepare_targets_for_gt_box_input-removeOutOfBox_wKps1-SUN10_RE MODEL.RCNN_WEIGHT_KPS_HEAD 0212-2047_mm2_SUN360RCNN_LossposeCamera_lr1e-5_NOsubsample-prepare_targets_for_gt_box_input-removeOutOfBox_wKps1-SUN10_RE'\\\n",
    "                       .split())\n",
    "\n",
    "print(opt)\n",
    "                        \n",
    "opt.checkpoints_folder = 'checkpoint'\n",
    "\n",
    "config_file = opt.config_file\n",
    "cfg.merge_from_file(config_file)\n",
    "# manual override some options\n",
    "cfg.merge_from_list([\"MODEL.DEVICE\", \"cuda\"])\n",
    "cfg.merge_from_list(opt.opts)\n",
    "cfg.freeze()\n",
    "\n",
    "opt.cfg = cfg\n",
    "\n",
    "# sys.path.insert(0, cfg.MODEL.POINTNET.PATH)\n",
    "opt.rank = opt.local_rank\n",
    "\n",
    "num_gpus = 1\n",
    "opt.distributed = num_gpus > 1\n",
    "device = 'cuda'\n",
    "opt.device = device\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# === SUMMARY WRITERS\n",
    "summary_path = './summary/'+opt.task_name\n",
    "writer = SummaryWriter(summary_path)\n",
    "\n",
    "# === LOGGING\n",
    "logger = setup_logger(\"logger:train\", summary_path, get_rank(), filename=\"logger_maskrcn-style.txt\")\n",
    "logger.info(colored(\"==[config]== opt\", 'white', 'on_blue'))\n",
    "logger.info(opt)\n",
    "logger.info(colored(\"==[config]== cfg\", 'white', 'on_blue'))\n",
    "logger.info(cfg)\n",
    "logger.info(colored(\"==[config]== Loaded configuration file {}\".format(opt.config_file), 'white', 'on_blue'))\n",
    "with open(opt.config_file, \"r\") as cf:\n",
    "    config_str = \"\\n\" + cf.read()\n",
    "    logger.info(config_str)\n",
    "printer = printer(get_rank(), debug=opt.debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MODEL & TRAINING\n",
    "modules_not_build = []\n",
    "if not opt.train_cameraCls:\n",
    "    modules_not_build.append('classifier_heads')\n",
    "if not opt.train_roi_h:\n",
    "    modules_not_build.append('roi_h_heads')\n",
    "if not opt.est_bbox and not opt.est_kps:\n",
    "    modules_not_build.append('roi_bbox_heads')\n",
    "sys.path.insert(0, 'models/pointnet')\n",
    "model = RCNNOnly_combine(opt, logger, printer, num_layers=opt.num_layers, modules_not_build=modules_not_build)\n",
    "\n",
    "# model.print_net()\n",
    "# model.init_restore()\n",
    "# model.set_train_params()\n",
    "model.to(device)\n",
    "model.turn_on_all_params()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    printer.print(name, param.shape, param.requires_grad)\n",
    "printer.print('ALL %d params'%len(list(model.named_parameters())))\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=cfg.SOLVER.BASE_LR, betas=(opt.beta1, 0.999), eps=1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=100, cooldown=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CHECKPOINT\n",
    "\n",
    "# [1] 20200221-120910_pod_firstKpsAndRest_batchsize8_KaimingInit_adam_wPerson05_wKPS1e-3_720-540_REafterDeathV_afterFaster_bs16_fix3_DISCOUNTfromPredkps__personLoss3Layers\n",
    "# opt.resume = '20200221-120910_pod_firstKpsAndRest_batchsize8_KaimingInit_adam_wPerson05_wKPS1e-3_720-540_REafterDeathV_afterFaster_bs16_fix3_DISCOUNTfromPredkps__personLoss3Layers'\n",
    "# [2.1] 20200222-162430_pod_backCompat_adam_wPerson05_720-540_REafterDeathV_afterFaster_bs16_fix3_nokpsLoss_personLoss3Layers_loss3layers\n",
    "# opt.resume = '20200222-162430_pod_backCompat_adam_wPerson05_720-540_REafterDeathV_afterFaster_bs16_fix3_nokpsLoss_personLoss3Layers_loss3layers'\n",
    "\n",
    "\n",
    "opt.checkpoints_path_task = os.path.join(opt.checkpoints_folder, opt.task_name)\n",
    "save_to_disk = get_rank() == 0\n",
    "checkpointer = DetectronCheckpointer(\n",
    "    opt, model, optimizer, scheduler, opt.checkpoints_folder, opt.checkpoints_path_task, save_to_disk, logger=logger\n",
    ")\n",
    "tid_start = 0\n",
    "epoch_start = 0\n",
    "if opt.resume != 'NoCkpt':\n",
    "    checkpoint_restored, _, _ = checkpointer.load(task_name=opt.resume)\n",
    "    if 'iteration' in checkpoint_restored:\n",
    "        tid_start = checkpoint_restored['iteration']\n",
    "    if 'epoch' in checkpoint_restored:\n",
    "        epoch_start = checkpoint_restored['epoch']\n",
    "    print(checkpoint_restored.keys())\n",
    "    logger.info(colored('Restoring from epoch %d - iter %d'%(epoch_start, tid_start), 'white', 'on_blue'))\n",
    "model.print_net()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DATASET\n",
    "train_trnfs_maskrcnn = build_transforms_maskrcnn(cfg, True)\n",
    "eval_trnfs_maskrcnn = build_transforms_maskrcnn(cfg, False)\n",
    "train_trnfs_yannick = build_transforms_yannick(cfg, True)\n",
    "eval_trnfs_yannick = build_transforms_yannick(cfg, False)\n",
    "\n",
    "ds_train_coco_vis = COCO2017Scale(transforms_yannick=train_trnfs_yannick, transforms_maskrcnn=train_trnfs_maskrcnn, split='train', shuffle=False, logger=logger, opt=opt) # !!!!!!!\n",
    "ds_eval_coco_vis = COCO2017Scale(transforms_yannick=eval_trnfs_yannick, transforms_maskrcnn=eval_trnfs_maskrcnn, split='val', shuffle=False, logger=logger, opt=opt) # !!!!!!!\n",
    "ds_test_coco_vis = COCO2017Scale(transforms_yannick=eval_trnfs_yannick, transforms_maskrcnn=eval_trnfs_maskrcnn, split='test', shuffle=False, logger=logger, opt=opt) # !!!!!!!\n",
    "\n",
    "training_loader_coco_vis = make_data_loader(\n",
    "    cfg,\n",
    "    ds_train_coco_vis,\n",
    "    is_train=False,\n",
    "    is_distributed=False,\n",
    "    start_iter=0,\n",
    "    logger=logger,\n",
    "    collate_fn=my_collate,\n",
    "    batch_size_override=1, # BN does not make sense when model.train() and batchsize==1!\n",
    ")\n",
    "eval_loader_coco_vis = make_data_loader(cfg, ds_eval_coco_vis, is_train=False, is_distributed=False, is_for_period=True, logger=logger, collate_fn=my_collate, batch_size_override=1)\n",
    "test_loader_coco_vis = make_data_loader(cfg, ds_test_coco_vis, is_train=False, is_distributed=False, is_for_period=True, logger=logger, collate_fn=my_collate, batch_size_override=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval-RELEASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = 'release_results'\n",
    "from pathlib import Path\n",
    "Path(results_path).mkdir(exist_ok=True)\n",
    "task_name = opt.resume\n",
    "task_name_appendix = '-predH'\n",
    "# task_name_appendix = '-fitH'\n",
    "task_name += task_name_appendix\n",
    "\n",
    "# task_name = 'tmp'\n",
    "print(task_name)\n",
    "\n",
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "write_folder = os.path.join(results_path, task_name)\n",
    "for subfolder in ['', 'png', 'npy', 'pickle', 'results']:\n",
    "    Path(os.path.join(write_folder, subfolder)).mkdir(parents=True, exist_ok=True)\n",
    "results_path_png = os.path.join(write_folder, 'png')\n",
    "results_path_results = os.path.join(write_folder, 'results')\n",
    "\n",
    "is_training = False\n",
    "if_vis = True\n",
    "if_debug = False\n",
    "prepostfix='testSet-|-evalMode'\n",
    "\n",
    "test_loader = training_loader_coco_vis\n",
    "# test_loader = eval_loader_coco_vis\n",
    "# test_loader = test_loader_coco_vis\n",
    "\n",
    "if_blender = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.zero_pitch = False\n",
    "\n",
    "# select_show = -1\n",
    "select_show = 0\n",
    "# select_show = 35, 38, 41 # large pitch\n",
    "# select_show = 35\n",
    "# select_show = 923 # 30 degree pitch\n",
    "# select_show = 12\n",
    "# select_show = 18\n",
    "# select_show = 15 # example of fitting person height\n",
    "\n",
    "from utils.model_utils import get_bins_combine\n",
    "import utils.model_utils as model_utils\n",
    "\n",
    "from utils.utils_misc import *\n",
    "from utils import utils_coco\n",
    "from utils.train_utils import f_pixels_to_mm\n",
    "import utils.geo_utils as geo_utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io as io\n",
    "from matplotlib.patches import Rectangle\n",
    "import utils.utils_coco as utils_coco\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "logger = setup_logger(\"logger:train\", summary_path, get_rank(), filename=\"logger_maskrcn-style.txt\")\n",
    "\n",
    "## START TRAINING\n",
    "best_loss = float('inf')\n",
    "bins = get_bins_combine(device)\n",
    "tid = 0\n",
    "\n",
    "epoch = 0\n",
    "eval_loss = 0\n",
    "\n",
    "model.eval()\n",
    "\n",
    "cpu_device = torch.device(\"cpu\")\n",
    "loss_func = torch.nn.L1Loss()\n",
    "if opt.distributed:\n",
    "    rank = dist.get_rank()\n",
    "else:\n",
    "    rank = 0\n",
    "    \n",
    "eval_loss_vt_list = []\n",
    "eval_loss_person_list = []\n",
    "vt_loss_allBoxes_dict = {}\n",
    "vt_loss_allBoxes_dict_list = []\n",
    "\n",
    "im_filename_list = []\n",
    "\n",
    "test_list = []\n",
    "\n",
    "vt_loss_all =[]\n",
    "\n",
    "num_plots = 0\n",
    "pitch_abs_list = []\n",
    "with torch.no_grad():\n",
    "    for i, (_, inputCOCO_Image_maskrcnnTransform_list, W_batch_array, H_batch_array, yc_batch, \\\n",
    "            bboxes_batch_array, bboxes_length_batch_array, v0_batch, f_pixels_yannick_batch, im_filename, im_file, target_maskrcnnTransform_list, labels_list) in tqdm(enumerate(test_loader)):\n",
    "        \n",
    "        if select_show != -1 and i < select_show:\n",
    "            continue\n",
    "        if_vis = True\n",
    "#         if i < 20:\n",
    "#             if_vis = True\n",
    "#         else:\n",
    "#             if_vis = False\n",
    "        tid = i\n",
    "#         print(i, im_filename)\n",
    "\n",
    "        input_dict = {'inputCOCO_Image_maskrcnnTransform_list': inputCOCO_Image_maskrcnnTransform_list, 'W_batch_array': W_batch_array, 'H_batch_array': H_batch_array, \\\n",
    "                      'yc_batch': yc_batch, \\\n",
    "                      'bboxes_batch_array': bboxes_batch_array, 'bboxes_length_batch_array': bboxes_length_batch_array, \\\n",
    "                      'v0_batch': v0_batch, 'f_pixels_yannick_batch': f_pixels_yannick_batch, 'im_filename': im_filename, 'im_file': im_file, \\\n",
    "                      'bins': bins, 'target_maskrcnnTransform_list': target_maskrcnnTransform_list, 'labels_list': labels_list}\n",
    "#         if opt.train_cameraCls:\n",
    "#             input_dict.update({\\\n",
    "#                       'inputSUN360_Image_maskrcnnTransform_list': inputSUN360_Image_maskrcnnTransform_list, 'im_paths_SUN360': im_paths_SUN360, \\\n",
    "#                       'pitch_dist_gt': pitch_dist_gt, 'roll_dist_gt': roll_dist_gt, 'vfov_dist_gt': vfov_dist_gt, 'horizon_dist_gt': horizon_dist_gt, 'metadata': metadata, \\\n",
    "#                       'pitch_num': pitch_list, 'roll_num': roll_list, 'vfov_num': vfov_list, 'horizon_num': horizon_list, 'f_num': focal_length_35mm_eq_list, 'sensor_size_num': sensor_size_list, \\\n",
    "#                         'W_list': W_list, 'H_list': H_list})\n",
    "\n",
    "        if_print = is_training\n",
    "        cfg = opt.cfg\n",
    "        bins = input_dict['bins']\n",
    "\n",
    "        # ========= Rui's inputs\n",
    "        inputCOCO_Image_maskrcnnTransform_list = input_dict['inputCOCO_Image_maskrcnnTransform_list']\n",
    "        bboxes_batch, v0_batch_offline, f_pixels_yannick_batch_offline, W_batch, H_batch, yc_batch_offline = \\\n",
    "            torch.from_numpy(input_dict['bboxes_batch_array']).float().to(device), input_dict['v0_batch'].to(device), input_dict['f_pixels_yannick_batch'].to(device), \\\n",
    "            torch.from_numpy(input_dict['W_batch_array']).to(device), torch.from_numpy(input_dict['H_batch_array']).to(device), input_dict['yc_batch'].to(device)\n",
    "\n",
    "        if not opt.not_rcnn:\n",
    "            list_of_bbox_list_cpu = model_utils.bboxArray_to_bboxList(bboxes_batch, input_dict['bboxes_length_batch_array'], input_dict['W_batch_array'], input_dict['H_batch_array'])\n",
    "        else:\n",
    "            list_of_bbox_list_cpu = []\n",
    "\n",
    "        list_of_oneLargeBbox_list_cpu = model_utils.oneLargeBboxList(input_dict['W_batch_array'], input_dict['H_batch_array']) # in original image size\n",
    "        list_of_oneLargeBbox_list = [bbox_list_array.to(device) for bbox_list_array in list_of_oneLargeBbox_list_cpu]\n",
    "\n",
    "\n",
    "        if if_vis:\n",
    "            input_dict_show = {'H': input_dict['H_batch_array'], 'W': input_dict['W_batch_array']}\n",
    "            if 'testSet' not in prepostfix:\n",
    "                input_dict_show.update({'v0_cocoPredict': input_dict['v0_batch'].numpy()})\n",
    "            input_dict_show['bbox_gt'] = []\n",
    "            input_dict_show['bbox_est'] = []\n",
    "            input_dict_show['bbox_fit'] = []\n",
    "            input_dict_show['bbox_h'] = []\n",
    "            input_dict_show['bbox_geo'] = []\n",
    "            input_dict_show['bbox_loss'] = []\n",
    "\n",
    "        input_dict_misc = {'bins': bins, 'is_training': is_training, 'H_batch': H_batch, 'W_batch': W_batch, 'bboxes_batch': bboxes_batch, 'loss_func': loss_func, \\\n",
    "                           'cpu_device': cpu_device,  'device': device, 'tid': tid, 'rank': rank, 'data': 'coco', 'if_vis': if_vis}\n",
    "        output_RCNN = model(input_dict_misc=input_dict_misc, input_dict=input_dict, image_batch_list=inputCOCO_Image_maskrcnnTransform_list, \\\n",
    "                            list_of_bbox_list_cpu=list_of_bbox_list_cpu, list_of_oneLargeBbox_list=list_of_oneLargeBbox_list, \\\n",
    "                            )\n",
    "        pitch_abs_degree = abs(output_RCNN['pitch_batch_est'][0].item()) / np.pi * 180.\n",
    "        pitch_abs_list.append(pitch_abs_degree)\n",
    "#         if pitch_abs_degree < 15:\n",
    "#             continue\n",
    "        if 'zero_pitch' in opt and opt.zero_pitch:\n",
    "            output_RCNN['pitch_batch_est'] *= 0.\n",
    "            output_RCNN['output_pitch'] *= 0.\n",
    "            output_RCNN['v0_batch_from_pitch_vfov'] = output_RCNN['v0_batch_from_pitch_vfov'] * 0. + H_batch[0] / 2.\n",
    "            output_RCNN['v0_batch_est'] = output_RCNN['v0_batch_from_pitch_vfov']\n",
    "            \n",
    "        output_horizon = output_RCNN['output_horizon']\n",
    "        output_pitch = output_RCNN['output_pitch']\n",
    "        output_vfov = output_RCNN['output_vfov']\n",
    "        output_yc_batch = output_RCNN['output_yc_batch']\n",
    "        f_pixels_yannick_batch_ori = f_pixels_yannick_batch_offline.clone()\n",
    "        v0_batch_ori = v0_batch_offline.clone()\n",
    "        \n",
    "        v0_batch_est = output_RCNN['v0_batch_est']\n",
    "        v0_batch_from_pitch_vfov = output_RCNN['v0_batch_from_pitch_vfov']\n",
    "\n",
    "\n",
    "        vfov_estim = output_RCNN['vfov_estim']\n",
    "        f_pixels_yannick_batch_est = output_RCNN['f_pixels_batch_est']\n",
    "\n",
    "        pitch_batch_est = output_RCNN['pitch_batch_est']\n",
    "        pitch_estim_yannick = output_RCNN['pitch_estim_yannick']\n",
    "\n",
    "        yc_est_batch = output_RCNN['yc_est_batch']\n",
    "\n",
    "        person_h_list = output_RCNN['person_h_list']\n",
    "        all_person_hs = output_RCNN['all_person_hs']\n",
    "\n",
    "        reduce_method = output_RCNN['reduce_method']\n",
    "        \n",
    "\n",
    "\n",
    "        if tid % opt.summary_every_iter == 0 and if_print:\n",
    "            f_mm_array_est = f_pixels_to_mm(f_pixels_yannick_batch_est, input_dict)\n",
    "\n",
    "        ## Losses for bbox fitting\n",
    "        vt_loss_sample_batch_list = []\n",
    "        vt_loss_allBoxes_dict_cpu = {}\n",
    "        vt_error_fit_allBoxes_dict_cpu = {}\n",
    "        camH_fit_batch = []\n",
    "\n",
    "        for idx, bboxes_length in enumerate(input_dict['bboxes_length_batch_array']):\n",
    "            bboxes = bboxes_batch[idx][:bboxes_length] # [N, 4]\n",
    "            # W = W_batch[idx]\n",
    "            H = H_batch[idx]\n",
    "            vc = H / 2.\n",
    "            v0_est = v0_batch_est[idx]\n",
    "            v0_ori = v0_batch_ori[idx]  # [top H, bottom 0]\n",
    "            pitch_est = pitch_batch_est[idx]\n",
    "            f_pixels_yannick_est = f_pixels_yannick_batch_est[idx]\n",
    "            inv_f2 = 1. / (f_pixels_yannick_est * f_pixels_yannick_est)\n",
    "            yc_est = yc_est_batch[idx]\n",
    "\n",
    "            H_np = H_batch[idx].cpu().numpy()\n",
    "            W_np = W_batch[idx].cpu().numpy()\n",
    "\n",
    "            if opt.not_rcnn:\n",
    "                h_human_s = torch.from_numpy(np.asarray([1.75] * bboxes.shape[0], dtype=np.float32)).float().to(device)\n",
    "                # h_human_s = [1.75] * bboxes.shape[0]\n",
    "            else:\n",
    "                h_human_s = person_h_list[idx] * output_RCNN['straighten_ratios_list'][idx]\n",
    "\n",
    "            vt_loss_sample_list = []\n",
    "            vt_error_sample_fit_list = []\n",
    "\n",
    "            # Fitting 现场: getting camera heights\n",
    "            camH_fit_list = []\n",
    "            for bbox_idx, bbox in enumerate(bboxes):\n",
    "                y_person_fit = 1.75\n",
    "                camH_fit_bbox = geo_utils.fit_camH(bbox.cpu(), H.cpu(), v0_est.cpu(), vc.cpu(), f_pixels_yannick_est.cpu(), y_person_fit)\n",
    "                camH_fit_list.append(camH_fit_bbox.detach().numpy())\n",
    "            camH_fit = np.median(np.array(camH_fit_list))\n",
    "            camH_fit_batch.append(camH_fit)\n",
    "\n",
    "            bbox_gt_sample = []\n",
    "            bbox_est_sample = []\n",
    "            bbox_fit_sample = []\n",
    "            bbox_h_sample = []\n",
    "            bbox_geo_sample = []\n",
    "            bbox_loss_sample = []\n",
    "            for bbox_idx, (bbox, y_person) in enumerate(zip(bboxes, h_human_s)):\n",
    "                vb = H - (bbox[1] + bbox[3]) # [top H bottom 0]\n",
    "                vt_gt = H - bbox[1] # [top H bottom 0]\n",
    "                \n",
    "                ## Fit y_person\n",
    "                geo_model_input_dict = {'yc_est': yc_est, 'vb': vb, 'y_person': y_person, 'v0': v0_est, 'vc': vc, 'f_pixels_yannick': f_pixels_yannick_est, 'pitch_est': pitch_est}\n",
    "                negative_z = False\n",
    "                if opt.accu_model:\n",
    "                    vt_camEst, z, negative_z = model_utils.accu_model_helanyi(geo_model_input_dict, if_debug=False)  # [top H bottom 0]\n",
    "                    print('======>>>>>Helanyi:', vt_camEst.item())\n",
    "                    geo_model_input_dict_rui = {'yc_est': yc_est, 'vb': vb, 'y_person': y_person*torch.cos(pitch_est), 'v0': v0_est, 'vc': vc, 'f_pixels_yannick': f_pixels_yannick_est, 'pitch_est': pitch_est}\n",
    "                    vt_camEst_rui, z_rui, negative_z_rui = model_utils.accu_model(geo_model_input_dict_rui, if_debug=False)  # [top H bottom 0]\n",
    "                    print('======>>>>>RUI----:', vt_camEst_rui.item())\n",
    "#                     geo_model_input_dict_rui = {'yc_est': yc_est, 'vb': vb, 'y_person': y_person, 'v0': v0_est, 'vc': vc, 'f_pixels_yannick': f_pixels_yannick_est, 'pitch_est': pitch_est}\n",
    "#                     vt_camEst_rui, z_rui, negative_z_rui = model_utils.accu_model_fixedz(geo_model_input_dict_rui, if_debug=False)  # [top H bottom 0]\n",
    "#                     print('======>>>>>RUI FIXEDZ----:', vt_camEst_rui.item())\n",
    "                else:\n",
    "                    vt_camEst = model_utils.approx_model(geo_model_input_dict)\n",
    "\n",
    "                vt_loss = loss_func(vt_gt, vt_camEst.reshape([])) / bbox[3]\n",
    "                vt_loss = torch.clamp(vt_loss, 0., opt.cfg.MODEL.LOSS.VT_LOSS_CLAMP)\n",
    "                if negative_z and if_debug:\n",
    "                    print('>>>>>>', negative_z, vt_loss)\n",
    "                # vt_loss = vt_loss * (not(negative_z))\n",
    "                vt_loss = torch.where(torch.isnan(vt_loss), torch.zeros_like(vt_loss), vt_loss)\n",
    "                if negative_z and if_debug:\n",
    "                    print('>>>>>>>>>>>', vt_loss)\n",
    "                vt_loss_sample_list.append(vt_loss)\n",
    "                vt_loss_allBoxes_dict_cpu.update({'bbox_vt_loss_tid%04d_rank%d_%02d-%02d'%(tid, rank, idx, bbox_idx): vt_loss.to(cpu_device)})\n",
    "\n",
    "                # Fitting 现场: getting vt\n",
    "                y_person_fit = 1.75\n",
    "                vt_camFit = geo_utils.fit_vt(camH_fit, vb, v0_est, vc, y_person_fit, 1. / (f_pixels_yannick_est  * f_pixels_yannick_est))\n",
    "                vt_error_fit = loss_func(vt_gt, vt_camFit) / bbox[3]\n",
    "\n",
    "                vt_error_fit_allBoxes_dict_cpu.update({'bbox_vt_error_fit_tid%04d_rank%d_%02d-%02d'%(tid, rank, idx, bbox_idx): vt_error_fit.to(cpu_device)})\n",
    "\n",
    "                vt_error_sample_fit_list.append(vt_error_fit.detach().cpu().numpy().item())\n",
    "\n",
    "                if if_vis:\n",
    "                    bbox_np = bbox.cpu().numpy()\n",
    "                    vt_camEst_np = vt_camEst.detach().cpu().numpy()\n",
    "                    vt_camFit_np = vt_camFit.detach().cpu().numpy()\n",
    "                    bbox_gt_sample.append([bbox_np[0], bbox_np[1], bbox_np[2], bbox_np[3]]) # [x, y (top), w, h]\n",
    "                    bbox_est_sample.append([bbox_np[0], H_np - vt_camEst_np, bbox_np[2], bbox_np[1]+bbox_np[3]-(H_np - vt_camEst_np)])\n",
    "                    bbox_h_sample.append(y_person.cpu().detach().numpy())\n",
    "                    bbox_geo_sample.append(geo_model_input_dict)\n",
    "                    bbox_loss_sample.append(vt_loss.item())\n",
    "\n",
    "            if if_vis:\n",
    "                input_dict_show['bbox_gt'].append(bbox_gt_sample)\n",
    "                input_dict_show['bbox_est'].append(bbox_est_sample)\n",
    "                input_dict_show['bbox_fit'].append(bbox_fit_sample)\n",
    "                input_dict_show['bbox_h'].append(bbox_h_sample)\n",
    "                input_dict_show['bbox_geo'].append(bbox_geo_sample)\n",
    "                input_dict_show['bbox_loss'].append(bbox_loss_sample)\n",
    "\n",
    "\n",
    "            vt_loss_sample = torch.mean(torch.stack(vt_loss_sample_list))\n",
    "            vt_loss_sample_batch_list.append(vt_loss_sample)\n",
    "\n",
    "        vt_loss_batch = torch.stack(vt_loss_sample_batch_list)\n",
    "        loss_vt = torch.mean(vt_loss_batch)\n",
    "\n",
    "        return_dict = {'vt_loss_batch': vt_loss_batch, 'vt_loss_allBoxes_dict': vt_loss_allBoxes_dict_cpu, 'vt_error_fit_allBoxes_dict': vt_error_fit_allBoxes_dict_cpu, \\\n",
    "                       'yc_est_batch': yc_est_batch, 'yc_batch_offline': yc_batch_offline, 'yc_fit_batch': np.array(camH_fit_batch)}\n",
    "        vt_loss_allBoxes_dict_list.append(vt_loss_allBoxes_dict_cpu)\n",
    "        if tid % opt.summary_every_iter == 0 and if_print:\n",
    "            return_dict.update({'f_mm_batch': f_mm_array_est.reshape(-1, 1)})\n",
    "\n",
    "        if 'loss_vt_list' in output_RCNN:\n",
    "            loss_vt_list = output_RCNN['loss_vt_list']\n",
    "            assert len(loss_vt_list) != 0\n",
    "        else:\n",
    "            loss_vt_list = []\n",
    "        loss_vt_list.append(loss_vt)\n",
    "        return_dict.update({'loss_vt_list': loss_vt_list})\n",
    "        loss_dict = {'loss_vt': sum(loss_vt_list)/len(loss_vt_list)} # mean of layers; for optimization and scheduler\n",
    "\n",
    "        if opt.pointnet_camH_refine:\n",
    "            loss_vt_layers_dict = {}\n",
    "            for loss_idx, loss in enumerate(loss_vt_list):\n",
    "                loss_vt_layers_dict['loss_vt_layer_%d'%(loss_idx-len(loss_vt_list))] = loss\n",
    "\n",
    "            return_dict.update({'loss_vt_layers_dict': loss_vt_layers_dict})\n",
    "\n",
    "\n",
    "        if not opt.not_rcnn:\n",
    "            return_dict.update({'all_person_hs': all_person_hs})\n",
    "            loss_all_person_h_list = output_RCNN['loss_all_person_h_list']\n",
    "            return_dict.update({'loss_all_person_h_list': loss_all_person_h_list})\n",
    "            loss_dict.update({'loss_person': sum(loss_all_person_h_list)/len(loss_all_person_h_list)}) # mean of layers; for optimization and scheduler\n",
    "\n",
    "\n",
    "        # ========== Some vis\n",
    "        if if_vis:\n",
    "            input_dict_show['W_batch_array'] = input_dict['W_batch_array']\n",
    "            input_dict_show['H_batch_array'] = input_dict['H_batch_array']\n",
    "            if opt.est_kps:    \n",
    "                input_dict_show['predictions'] = output_RCNN['predictions']\n",
    "                input_dict_show['target_maskrcnnTransform_list'] = input_dict['target_maskrcnnTransform_list']\n",
    "#             input_dict_show['v0_batch_predict'] = v0_batch_predict.detach().cpu().numpy()  # (H = top of the image, 0 = bottom of the image)\n",
    "            input_dict_show['v0_batch_from_pitch_vfov'] = v0_batch_from_pitch_vfov.detach().cpu().numpy()\n",
    "            input_dict_show['v0_batch_est'] = v0_batch_est.detach().cpu().numpy()\n",
    "            if 'v0_batch_est_0' in output_RCNN:\n",
    "                input_dict_show['v0_batch_est_0'] = output_RCNN['v0_batch_est_0'].detach().cpu().numpy()\n",
    "            f_pixels_yannick_single_est = f_pixels_yannick_batch_est.detach().cpu().numpy()\n",
    "            f_pixels_yannick_single_est_mm = [utils_coco.fpix_to_fmm(f_pixels_yannick_single_est_0, H_np, W_np) for f_pixels_yannick_single_est_0 in f_pixels_yannick_single_est]\n",
    "            f_pixels_yannick_single_ori = f_pixels_yannick_batch_ori.detach().cpu().numpy()\n",
    "            f_pixels_yannick_single_ori_mm = [utils_coco.fpix_to_fmm(f_pixels_yannick_single_ori_0, H_np, W_np) for f_pixels_yannick_single_ori_0 in f_pixels_yannick_single_ori]\n",
    "            input_dict_show.update({'yc_fit': yc_batch_offline.detach().cpu().numpy(), 'yc_est': yc_est_batch.detach().cpu().numpy()})\n",
    "\n",
    "            if len(output_RCNN['yc_est_batch_np_list']) > 1: # more than one layers\n",
    "                input_dict_show.update({'yc_est_list': output_RCNN['yc_est_batch_np_list']})\n",
    "            if len(output_RCNN['person_hs_est_np_list']) > 1:\n",
    "                input_dict_show.update({'person_hs_est_list': output_RCNN['person_hs_est_np_list']})\n",
    "            if 'vt_camEst_N_delta_np_list' in output_RCNN and len(output_RCNN['vt_camEst_N_delta_np_list']) > 1:\n",
    "                input_dict_show.update({'vt_camEst_N_delta_est_list': output_RCNN['vt_camEst_N_delta_np_list']})\n",
    "            if len(output_RCNN['f_pixels_est_batch_np_list']) > 1: # more than one layers\n",
    "                input_dict_show.update({'f_pixels_est_mm_list': [utils_coco.fpix_to_fmm(f_pixels_est_0, H_np, W_np) for f_pixels_est_0 in output_RCNN['f_pixels_est_batch_np_list']]})\n",
    "            if len(output_RCNN['v0_01_est_batch_np_list']) > 1: # more than one layers\n",
    "                input_dict_show.update({'v0_est_list': output_RCNN['v0_01_est_batch_np_list']})\n",
    "\n",
    "            input_dict_show.update({'f_est_px': f_pixels_yannick_single_est, 'f_est_mm': f_pixels_yannick_single_est_mm})\n",
    "            input_dict_show.update({'f_cocoPredict': f_pixels_yannick_single_ori, 'f_cocoPredict_mm': f_pixels_yannick_single_ori_mm})\n",
    "            input_dict_show.update({'pitch_est_angle': pitch_batch_est.detach().cpu().numpy()/np.pi*180.})\n",
    "\n",
    "            input_dict_show['im_path'] = list(input_dict['im_file'])\n",
    "            input_dict_show['im_filename'] = list(input_dict['im_filename'])\n",
    "            num_samples = len(input_dict['im_file'])\n",
    "            input_dict_show['output_horizon_COCO'] = output_RCNN['output_horizon'].detach().cpu().numpy()\n",
    "            input_dict_show['horizon_bins'] = [bins['horizon_bins_centers_torch'].cpu().numpy()] * num_samples\n",
    "\n",
    "            if not opt.direct_camH:\n",
    "                input_dict_show['output_camH_COCO'] = output_RCNN['output_yc_batch'].detach().cpu().numpy()\n",
    "                input_dict_show['camH_bins'] = [bins['yc_bins_centers_torch'].cpu().numpy()] * num_samples\n",
    "\n",
    "            input_dict_show['tid'] = [tid] * num_samples\n",
    "            input_dict_show['task_name'] = [opt.task_name] * num_samples\n",
    "            input_dict_show['num_samples'] = num_samples\n",
    "            input_dict_show['reduce_method'] = [reduce_method] * num_samples\n",
    "            input_dict_show.update({'vfov_est': vfov_estim.detach().cpu().numpy(), 'pitch_est_yannick': pitch_estim_yannick.detach().cpu().numpy()})\n",
    "\n",
    "            if input_dict_show['num_samples'] > 0:\n",
    "                input_dict_show_list = batch_dict_to_list_of_dicts(input_dict_show)\n",
    "                input_dict_show = input_dict_show_list[0]\n",
    "\n",
    "        prefix, postfix = prepostfix.split('|')\n",
    "        \n",
    "        save_path_blender = results_path_png\n",
    "        save_path_ours = results_path_png\n",
    "        prefix = 'coco_' + prefix\n",
    "        postfix += '_reproj'\n",
    "        \n",
    "        if if_vis:\n",
    "            vis_utils.show_cam_bbox(io.imread(input_dict_show['im_path']), input_dict_show, figzoom=1.5, if_show=True, \\\n",
    "                                    if_save=True if save_path_ours else False, if_pause=False, save_path=save_path_ours, save_name=prefix+'%06d'%(tid)+postfix+'_reproj')\n",
    "            if opt.est_kps:\n",
    "                vis_utils.show_box_kps(opt, model, io.imread(input_dict_show['im_path']), input_dict_show, if_show=True, if_pause=False, save_path='', save_name=prefix+'tid%d'%(tid)+postfix)\n",
    "            \n",
    "            if if_blender:\n",
    "                tmp_code = ''.join([random.choice(string.ascii_letters + string.digits) for n in range(32)])\n",
    "                blender_render(input_dict_show, im_file, save_path=save_path_ours, if_show=True, idx=tid, save_name=prefix+'%06d'%(tid)+postfix+'_render', tmp_code=tmp_code, \\\n",
    "                              render_type='cylinder', pick=-1, grid=False)\n",
    "        num_plots += 1\n",
    "        if num_plots >=5:\n",
    "            break\n",
    "\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up Blender for rendering virtual objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [!!!!!!] Change to your path of Blender binary!\n",
    "blender_path = '/home/ruizhu/Downloads/blender-2.79b-linux-glibc219-x86_64/blender'\n",
    "\n",
    "if_blender = True\n",
    "\n",
    "def blender_render(input_dict_show, im_file, save_path='./', if_show=False, idx=0, save_name='', tmp_code='iamgroot', \\\n",
    "                   render_type='cylinder', if_compact=False, pick=-1, grid=False):\n",
    "    assert render_type in ['chair', 'cylinder']\n",
    "    scene_path = current_dir + '/rendering/scene_chair_fix.blend'\n",
    "    if render_type == 'chair':\n",
    "        script_path = current_dir + '/rendering/render_coco_rui_chair_all_fix_final.py'\n",
    "    else:\n",
    "        script_path = current_dir + '/rendering/render_coco_rui_cylinder_all_fix_final.py'\n",
    "    \n",
    "    W = W_batch_array[0]\n",
    "    H = H_batch_array[0]\n",
    "    \n",
    "    insertion_points_xy_list = []\n",
    "    bboxes_filter = input_dict_show['bbox_gt']\n",
    "    # bboxes_filter = [bboxes_filter[0]]\n",
    "    bbox_hs_list = [a.item() for a in input_dict_show['bbox_h']]\n",
    "    for bbox in bboxes_filter:\n",
    "        insertion_points_xy_list.append([bbox[0]+bbox[2]/2., bbox[1]+bbox[3]])\n",
    "#         insertion_points_xy_list.append([bbox[0], bbox[1]+bbox[3]])\n",
    "    \n",
    "    if pick != -1:\n",
    "        insertion_points_xy_list = [insertion_points_xy_list[pick]]\n",
    "        bbox_hs_list = [bbox_hs_list[pick]]\n",
    "    \n",
    "    u_start_end = [W/4., W/4.*3.]\n",
    "    u_grid_size = (u_start_end[1] - u_start_end[0]) / 2.\n",
    "    v_start_end = [H - input_dict_show['v0_batch_from_pitch_vfov']+10, H]\n",
    "    # v_start_end = [0., H]\n",
    "    v_grid_size = (v_start_end[1] - v_start_end[0]) / 4.\n",
    "    \n",
    "    if grid:\n",
    "        for u in range(3):\n",
    "            for v in range(5):\n",
    "        #         if v > 0:\n",
    "        #             continue\n",
    "                insertion_points_xy_list.append([u_start_end[0] + u_grid_size * u, v_start_end[0] + v_grid_size * v])\n",
    "                bbox_hs_list += [2.5 / (v+1)]\n",
    "            \n",
    "    # insertion_points_xy_list = insertion_points_xy_list[:1]\n",
    "    tmp_dir = Path('rendering/tmp_dir')\n",
    "    tmp_dir.mkdir(exist_ok=True)\n",
    "    npy_path = tmp_dir / ('tmp_insert_pts_'+tmp_code)\n",
    "    np.save(str(npy_path), insertion_points_xy_list)\n",
    "    npy_path = tmp_dir / ('tmp_bbox_hs_'+tmp_code)\n",
    "    np.save(str(npy_path), bbox_hs_list)\n",
    "\n",
    "    im_filepath = im_file[0]\n",
    "    insertion_points_x = -1\n",
    "    insertion_points_y = -1\n",
    "    ppitch = output_RCNN['pitch_batch_est'].cpu().numpy()[0]\n",
    "    ffpixels = output_RCNN['f_pixels_batch_est'].cpu().numpy()[0]\n",
    "    vvfov = output_RCNN['vfov_estim'].cpu().numpy()[0]\n",
    "    hhfov = np.arctan(W / 2. / ffpixels) * 2.\n",
    "    h_cam = output_RCNN['yc_est_batch'].cpu().numpy()[0]\n",
    "\n",
    "    npy_path = Path(current_dir) / tmp_dir\n",
    "    rendering_command = '%s %s --background --python %s'%(blender_path, scene_path, script_path)\n",
    "    rendering_command_append = ' -- -npy_path %s -img_path %s -tmp_code %s -H %d -W %d -insertion_points_x %d -insertion_points_y %d -pitch %.6f -fov_h %.6f -fov_v %.6f -cam_h %.6f'%\\\n",
    "        (str(npy_path), im_filepath, tmp_code, H, W, insertion_points_x, insertion_points_y, ppitch, hhfov, vvfov, h_cam)\n",
    "    rendering_command = rendering_command + rendering_command_append\n",
    "    print(rendering_command)\n",
    "\n",
    "    os.system(rendering_command)\n",
    "    \n",
    "    if if_show == False:\n",
    "        plt.ioff()\n",
    "    fig = plt.figure(figsize=(15, 15), frameon=False)\n",
    "    def full_frame(width=None, height=None):\n",
    "        import matplotlib as mpl\n",
    "        mpl.rcParams['savefig.pad_inches'] = 0\n",
    "        figsize = None if width is None else (width, height)\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        ax = plt.axes([0,0,1,1], frameon=False)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        plt.autoscale(tight=True)\n",
    "    if if_compact:\n",
    "        full_frame(15, 15)\n",
    "    render_file = current_dir + '/rendering/render/render_all_%s.png'%tmp_code\n",
    "    im_render = plt.imread(render_file)\n",
    "    plt.imshow(im_render)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    input_dict = input_dict_show\n",
    "    if 'bbox_gt' in input_dict:\n",
    "        for bbox in input_dict['bbox_gt']:\n",
    "            rect = Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], linewidth=2, edgecolor='lime', facecolor='none')\n",
    "            print((bbox[0], bbox[1]+bbox[3]))\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "\n",
    "    if 'bbox_est' in input_dict:\n",
    "        for bbox in input_dict['bbox_est']:\n",
    "            rect = Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], linewidth=2, edgecolor='b', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "    v0_batch_from_pitch_vfov = input_dict_show['v0_batch_from_pitch_vfov']\n",
    "    plt.plot([0., W], [H - v0_batch_from_pitch_vfov, H - v0_batch_from_pitch_vfov], linestyle='-.', linewidth=2, color='blue')\n",
    "\n",
    "    for y_person, bbox in zip(input_dict['bbox_h'], input_dict['bbox_gt']):\n",
    "        plt.text(bbox[0], bbox[1], '%.2f'%(y_person), fontsize=10, bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "    \n",
    "    if if_compact:\n",
    "        ax.set_axis_off()\n",
    "        plt.axis('off')\n",
    "    plt.xlim([0, W])\n",
    "    plt.ylim([H, 0])\n",
    "    plt.autoscale(tight=True) \n",
    "    \n",
    "    plt.savefig(save_path + '/%s.jpg'%(save_name), dpi = 100, bbox_inches='tight')\n",
    "    \n",
    "    if if_show:\n",
    "        plt.show()\n",
    "        \n",
    "    plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
